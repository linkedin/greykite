{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Benchmarking\n\nYou can easily compare predictive performance of multiple algorithms such as\n``Silverkite`` and ``Prophet`` using the\n`~greykite.framework.benchmark.benchmark_class.BenchmarkForecastConfig` class.\nIn this tutorial we describe the step-by-step process of defining, running and monitoring a benchmark.\nWe also demonstrate how to use the class functions to compute and plot errors for multiple models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from dataclasses import replace\n\nimport plotly\nimport plotly.graph_objects as go\n\nfrom greykite.common.evaluation import EvaluationMetricEnum\nfrom greykite.framework.benchmark.benchmark_class import BenchmarkForecastConfig\nfrom greykite.framework.benchmark.data_loader_ts import DataLoaderTS\nfrom greykite.framework.templates.autogen.forecast_config import ComputationParam\nfrom greykite.framework.templates.autogen.forecast_config import EvaluationMetricParam\nfrom greykite.framework.templates.autogen.forecast_config import EvaluationPeriodParam\nfrom greykite.framework.templates.autogen.forecast_config import MetadataParam\nfrom greykite.framework.templates.autogen.forecast_config import ForecastConfig\nfrom greykite.framework.templates.autogen.forecast_config import ModelComponentsParam\nfrom greykite.framework.templates.model_templates import ModelTemplateEnum\nfrom greykite.sklearn.cross_validation import RollingTimeSeriesSplit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the data\nFirst load your dataset into a pandas dataframe.\nWe will use the peyton-manning dataset as a running example.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Loads dataset into UnivariateTimeSeries\ndl = DataLoaderTS()\nts = dl.load_peyton_manning_ts()\ndf = ts.df  # cleaned pandas.DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the Configs\nWe specify the models we want to benchmark via the ``configs`` parameter.\nIn this example we will benchmark 1 ``Prophet`` and 2 different ``Silverkite`` models.\nWe first define the common components of the models\nsuch as ``MetadataParam`` and ``EvaluationMetricParam``, and then update the configuration to specify\nindividual models.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "## Define common components  of the configs\n# Specifies dataset information\nmetadata = MetadataParam(\n    time_col=\"ts\",   # name of the time column\n    value_col=\"y\",   # name of the value column\n    freq=\"D\"         # \"H\" for hourly, \"D\" for daily, \"W\" for weekly, etc.\n)\n\n# Defines number of periods to forecast into the future\nforecast_horizon = 7\n\n# Specifies intended coverage of the prediction interval\ncoverage = 0.95\n\n# Defines the metrics to evaluate the forecasts\n# We use Mean Absolute Percent Error (MAPE) in this tutorial\nevaluation_metric = EvaluationMetricParam(\n    cv_selection_metric=EvaluationMetricEnum.MeanAbsolutePercentError.name,\n    cv_report_metrics=None\n)\n\n# Defines the cross-validation config within pipeline\nevaluation_period = EvaluationPeriodParam(\n    cv_max_splits=1,  # Benchmarking n_splits is defined in tscv, here we don't need split to choose parameter sets\n    periods_between_train_test=0,\n)\n\n# Defines parameters related to grid-search computation\ncomputation = ComputationParam(\n    hyperparameter_budget=None,\n    n_jobs=-1,  # to debug, change to 1 for more informative error messages\n    verbose=3)\n\n# Defines common components across all the configs\n# ``model_template`` and ``model_components_param`` changes between configs\ncommon_config = ForecastConfig(\n    metadata_param=metadata,\n    forecast_horizon=forecast_horizon,\n    coverage=coverage,\n    evaluation_metric_param=evaluation_metric,\n    evaluation_period_param=evaluation_period,\n    computation_param=computation,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we update ``common_config`` to specify the individual models.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Defines ``Prophet`` model template with custom seasonality\nmodel_components = ModelComponentsParam(\n    seasonality={\n            \"seasonality_mode\": [\"additive\"],\n            \"yearly_seasonality\": [\"auto\"],\n            \"weekly_seasonality\": [True],\n        },\n        growth={\n            \"growth_term\": [\"linear\"]\n        }\n)\nparam_update = dict(\n    model_template=ModelTemplateEnum.PROPHET.name,\n    model_components_param=model_components\n)\nProphet = replace(common_config, **param_update)\n\n# Defines ``Silverkite`` model template with automatic autoregression\n# and changepoint detection\nmodel_components = ModelComponentsParam(\n    changepoints={\n        \"changepoints_dict\": {\n            \"method\": \"auto\",\n        }\n    },\n    autoregression={\n        \"autoreg_dict\": \"auto\"\n    }\n)\nparam_update = dict(\n    model_template=ModelTemplateEnum.SILVERKITE.name,\n    model_components_param=model_components\n)\nSilverkite_1 = replace(common_config, **param_update)\n\n# Defines ``Silverkite`` model template via string encoding\nparam_update = dict(\n    model_template=\"DAILY_SEAS_NMQM_GR_LINEAR_CP_NM_HOL_SP2_FEASET_AUTO_ALGO_RIDGE_AR_AUTO_DSI_AUTO_WSI_AUTO\",\n    model_components_param=None\n)\nSilverkite_2 = replace(common_config, **param_update)\n\n# Define the list of configs to benchmark\n# The dictionary keys will be used to store the benchmark results\nconfigs = {\n    \"Prophet\": Prophet,\n    \"SK_1\": Silverkite_1,\n    \"SK_2\": Silverkite_2,\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the Cross-Validation (CV)\nIn time-series forecasting we use a Rolling Window CV.\nYou can easily define it by using\n`~greykite.sklearn.cross_validation.RollingTimeSeriesSplit` class.\nThe CV parameters depend on the data frequency,\nforecast horizon as well as the speed of the models.\nSee ``Benchmarking documentation`` for guidance on how\nto choose CV parameters for your use case.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define the benchmark folds\n# CV parameters are changed for illustration purpose\ntscv = RollingTimeSeriesSplit(\n    forecast_horizon=forecast_horizon,\n    min_train_periods=2 * 365,\n    expanding_window=True,\n    use_most_recent_splits=True,\n    periods_between_splits=5,\n    periods_between_train_test=0,\n    max_splits=4)  # reduced to 4 from 16 for faster runtime\n\n# Print the train, test split for benchmark folds\nfor split_num, (train, test) in enumerate(tscv.split(X=df)):\n    print(split_num, train, test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the Benchmark\nTo start the benchmarking procedure execute its ``run`` method.\n\nIf you get an error message at this point, then there is a compatibility issue between your\nbenchmark inputs. Check `Debugging the Benchmark` section for instructions on how to derive valid inputs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bm = BenchmarkForecastConfig(df=df, configs=configs, tscv=tscv)\nbm.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Monitor the Benchmark\nDuring benchmarking a couple of color coded progress bars are displayed to inform the user of the\nadvancement of the entire process. The first bar displays ``config`` level information, while\nthe second bar displays split level information for the current ``config``.\nSee example in `Benchmarking documentation`.\n\nOn the left side of the progress bar, it shows which ``config``/ split is currently being\nbenchmarked and progress within that level as a percentage.\n\nOn the right side, the user can see how many ``configs``/ splits have been benchmarked\nand how many are remaining. Additionally, this bar also displays elapsed time and remaining runtime\nfor the corresponding level.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benchmark Output\nThe output of a successful benchmark procedure is stored as a nested dictionary under the class attribute\n``result``. For details on the structure of this tree check\n``Benchmarking documentation``.\n\nYou can extract any specific information by navigating this tree. For example, you can\ncheck the summary and component plot of any ``config``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Check summary of SK_1 model on first fold\nmodel = bm.result[\"SK_2\"][\"rolling_evaluation\"][\"split_0\"][\"pipeline_result\"].model\nmodel[-1].summary(max_colwidth=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Check component plot of SK_2 on second fold\nmodel = bm.result[\"SK_2\"][\"rolling_evaluation\"][\"split_1\"][\"pipeline_result\"].model\nfig = model[-1].plot_components()\nplotly.io.show(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compare forecasts\nTo obtain forecasts run the ``extract_forecasts`` method. You only need to run this once.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bm.extract_forecasts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This method does two things.\n\n* For every ``config``, it gathers forecast results across rolling windows and stores it\n  as a dataframe in ``rolling_forecast_df`` under the ``config`` key. This helps in comparing forecasts\n  and prediction accuracy across splits for the ``config``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Forecast across rolling windows for SK_1\nforecast_sk_1 = bm.result[\"SK_1\"][\"rolling_forecast_df\"]\nforecast_sk_1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Concatenates ``rolling_forecast_df`` for all the ``configs`` and stores it as a dataframe in the\n  class attribute ``forecasts``. This helps in comparing forecasts and prediction accuracies across ``configs``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Forecasts across configs\nbm.forecasts.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For any ``config`` you can plot forecasts across splits. This allows you to quickly check if there is\nany particular time window where the test performance drops. The forecasts for adjacent folds will\noverlap if the time windows of the corresponding folds overlap.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = bm.plot_forecasts_by_config(config_name=\"SK_1\")\nplotly.io.show(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The importance of this function becomes more significant when assessing a models performance over a\nlonger period e.g. a year or multiple years. You can quickly catch if models test performance drops\nduring weekends, specific months or holiday seasons.\n\nYou can also compare forecasts from multiple ``configs`` by ``forecast_step`` which is\ndefined as any number between 1 and ``forecast_horizon``. This is useful in forecasts with longer\nforecast horizons to check if the forecast volatility changes over time.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = bm.plot_forecasts_by_step(forecast_step=3)\nplotly.io.show(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compare Errors\nYou can compare the predictive performance of your models via multiple evaluation metrics.\nIn this example we will use MAPE and RMSE, but you can use any metric from ``EvaluationMetricEnum``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "metric_dict = {\n    \"MAPE\": EvaluationMetricEnum.MeanAbsolutePercentError,\n    \"RMSE\": EvaluationMetricEnum.RootMeanSquaredError\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Non Grouping Errors\nTo compare evaluation metrics without any grouping use ``get_evaluation_metrics``.\nThe output shows metric values by ``config`` and ``split``. We can group by ``config_name`` to get\nmetric values aggregated across all folds.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Compute evaluation metrics\nevaluation_metrics_df = bm.get_evaluation_metrics(metric_dict=metric_dict)\n# Aggregate by model across splits\nerror_df = evaluation_metrics_df.drop(columns=[\"split_num\"]).groupby(\"config_name\").mean()\nerror_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Visualize\nfig = bm.plot_evaluation_metrics(metric_dict)\nplotly.io.show(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train MAPE is high because some values in training dataset are close to 0.\n\nYou can also compare the predictive accuracy across splits for any model from ``configs``.\nThis allows you to check if the model performance varies significantly across time periods.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Compute evaluation metrics for a single config\nevaluation_metrics_df = bm.get_evaluation_metrics(metric_dict=metric_dict, config_names=[\"SK_1\"])\n# Aggregate by split number\nerror_df = evaluation_metrics_df.groupby(\"split_num\").mean()\nerror_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Visualize\ntitle = \"Average evaluation metric across rolling windows\"\ndata = []\n# Each row (index) is a config. Adds each row to the bar plot.\nfor index in error_df.index:\n    data.append(\n        go.Bar(\n            name=index,\n            x=error_df.columns,\n            y=error_df.loc[index].values\n        )\n    )\nlayout = go.Layout(\n    xaxis=dict(title=None),\n    yaxis=dict(title=\"Metric Value\"),\n    title=title,\n    title_x=0.5,\n    showlegend=True,\n    barmode=\"group\",\n)\nfig = go.Figure(data=data, layout=layout)\nplotly.io.show(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Grouping Errors\nTo compare evaluation metrics with grouping use ``get_grouping_evaluation_metrics``.\nThis allows you to group the error values by time features such as day of week, month etc.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Compute grouped evaluation metrics\ngrouped_evaluation_df = bm.get_grouping_evaluation_metrics(\n    metric_dict=metric_dict,\n    which=\"test\",\n    groupby_time_feature=\"str_dow\")\n# Aggregate by split number\nerror_df = grouped_evaluation_df.groupby([\"str_dow\", \"config_name\"]).mean()\nerror_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Visualize\nfig = bm.plot_grouping_evaluation_metrics(\n    metric_dict=metric_dict,\n    which=\"test\",\n    groupby_time_feature=\"str_dow\")\nplotly.io.show(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see all the models have higher MAPE and RMSE during weekends. That means adding\n``is_weekend`` indicator to the models will help.\n\n### Compare runtimes\nYou can compare and visualize runtimes of the models using the following codes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Compute runtimes\nruntime_df = bm.get_runtimes()\n# Aggregate across splits\nruntimes_df = runtime_df.drop(columns=[\"split_num\"]).groupby(\"config_name\").mean()\nruntimes_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Visualize\nfig = bm.plot_runtimes()\nplotly.io.show(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can see ``Silverkite`` models run almost 3 times faster compared to ``Prophet``.\n\n## Debugging the Benchmark\nWhen the `run` method is called, the input ``configs`` are first assessed of\ntheir suitability for a cohesive benchmarking procedure via the ``validate`` method.\nThis is done prior to passing the ``configs`` to the forecasting pipeline to save wasted\ncomputing time for the user.\nThough not necessary, the user is encouraged to use ``validate`` for debugging.\n\nThe ``validate`` method runs a series of checks to ensure that\n\n* The ``configs`` are compatible among themselves. For example, it checks if all the ``configs``\n  have the same ``forecast horizon``.\n* The ``configs`` are compatible with the CV schema. For example, ``forecast_horizon`` and\n  ``periods_between_train_test`` parameters of ``configs`` are\n  matched against that of the ``tscv``.\n\nNote that the ``validate`` method does not guarantee that the models will execute properly\nwhile in the pipeline. It is a good idea to do a test run on a smaller data and/ or smaller\nnumber of splits before running the full procedure.\n\nIn the event of a mismatch a ``ValueError`` is raised with informative error messages\nto help the user in debugging. Some examples are provided below.\n\n### Error due to incompatible model components in config\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# regressor_cols is not part of Prophet's model components\nmodel_components=ModelComponentsParam(\n    regressors={\n        \"regressor_cols\": [\"regressor1\", \"regressor2\", \"regressor_categ\"]\n    }\n)\ninvalid_prophet = replace(Prophet, model_components_param=model_components)\ninvalid_configs = {\"invalid_prophet\": invalid_prophet}\nbm = BenchmarkForecastConfig(df=df, configs=invalid_configs, tscv=tscv)\ntry:\n    bm.validate()\nexcept ValueError as err:\n    print(err)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Error due to wrong template name\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# model template name is not part of TemplateEnum, thus invalid\nunknown_template = replace(Prophet, model_template=\"SOME_TEMPLATE\")\ninvalid_configs = {\"unknown_template\": unknown_template}\nbm = BenchmarkForecastConfig(df=df, configs=invalid_configs, tscv=tscv)\ntry:\n    bm.validate()\nexcept ValueError as err:\n    print(err)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Error due to different forecast horizons in configs\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# the configs are valid by themselves, however incompatible for\n# benchmarking as these have different forecast horizons\nProphet_forecast_horizon_30 = replace(Prophet, forecast_horizon=30)\ninvalid_configs = {\n    \"Prophet\": Prophet,\n    \"Prophet_30\": Prophet_forecast_horizon_30\n}\nbm = BenchmarkForecastConfig(df=df, configs=invalid_configs, tscv=tscv)\ntry:\n    bm.validate()\nexcept ValueError as err:\n    print(err)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Error due to different forecast horizons in config and tscv\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "## Error due to different forecast horizons in config and tscv\ntscv = RollingTimeSeriesSplit(forecast_horizon=15)\nbm = BenchmarkForecastConfig(df=df, configs=configs, tscv=tscv)\ntry:\n    bm.validate()\nexcept ValueError as err:\n    print(err)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}