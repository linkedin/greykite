{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Tune your first anomaly detection model\n\nThis is a basic tutorial for creating and tuning a Greykite AD (Anomaly Detection) model.\nIt is intended for users who are new to Greykite AD and want to get started quickly.\n\nThe Greykite AD is a forecast-based AD method i.e. the forecast is used as the baseline.\nA data point is predicted as anomalous if it is outside the forecasted confidence intervals.\nThe Greykite AD algorithm gives you the flexibility to better model and control the\nconfidence intervals. A forecast based AD method is inherently dependent on an accurate\nforecasting model to achieve satisfactory AD performance.\n\nThroughout this tutorial, we will assume that you are familiar with tuning a\nGreykite forecast model. If you are not, please refer to the\n:doc:`/gallery/tutorials/0100_forecast_tutorial`.\n\nThe anomaly detection config (``ADConfig``) allows the users divide the time series into segments and\nlearn a different volatility model for each segment. The user can specify the volatility features.\nIt also allows users to specify objective function, constraints and parameter space to optimize the\nconfidence intervals.\n\nThese features include:\n\n    Volatility Features:\n        This allows users to specify the features to segment the time series and learn a\n        different volatility model for each segment. For example, if the time series is a daily\n        time series, the user can specify the volatility features as ``[\"dow\"]`` to learn a\n        different volatility model for each day of the week. The user can also specify multiple\n        volatility features. For example, if the time series is a daily time series, the user can\n        specify the volatility features as ``[[\"dow\", \"is_weekend\"]]`` to learn a different\n        volatility model for each day of the week and a different volatility model for weekends.\n\n    Coverage Grid:\n        This allows users to specify a grid of the confidence intervals. The ``coverage_grid`` is\n        specified as a list of floats between 0 and 1. For example, if the ``coverage_grid`` is specified as\n        ``[0.5, 0.95]``, the algorithm optimizes over confidence intervals with coverage ``0.5`` and ``0.95``.\n\n    Target Anomaly Percentage:\n        This allows users to specify the ``target_anomaly_percent``, which is specified as a float\n        between 0 and 1. For example, if ``target_anomaly_percent`` is\n        specified as ``0.1``, the anomaly score threshold is optimized such that 10% of the data\n        points are predicted as anomalous.\n\n    Target Precision:\n        This allows users to specify the ``target_precision``, which is specified as a\n        float between 0 and 1. For example, if the ``target_precision`` is specified as ``0.9``, the\n        anomaly score threshold is optimized such that at least 90% of the predicted anomalies are true\n        anomalies. This is useful when the user has a limited budget to investigate the anomalies.\n\n    Target Recall:\n        This allows users to specify the ``target_recall``, which is specified as a float\n        between 0 and 1. For example, if the ``target_recall`` is specified as ``0.9``, the anomaly\n        score threshold is optimized such that at least 90% of the true anomalies are predicted as\n        anomalies. This is useful when the user wants to detect most of the anomalies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import datetime\n\nimport numpy as np\nimport pandas as pd\nimport plotly\nimport plotly.express as px\nfrom greykite.common.constants import ANOMALY_COL\nfrom greykite.common.constants import TIME_COL\nfrom greykite.common.constants import VALUE_COL\nfrom greykite.common.testing_utils import generate_df_for_tests\nfrom greykite.common.testing_utils_anomalies import contaminate_df_with_anomalies\nfrom greykite.common.viz.timeseries_annotate import plot_lines_markers\nfrom greykite.detection.common.ad_evaluation import f1_score\nfrom greykite.detection.common.ad_evaluation import precision_score\nfrom greykite.detection.common.ad_evaluation import recall_score\nfrom greykite.detection.detector.ad_utils import partial_return\nfrom greykite.detection.detector.config import ADConfig\nfrom greykite.detection.detector.data import DetectorData\nfrom greykite.detection.detector.greykite import GreykiteDetector\nfrom greykite.detection.detector.reward import Reward\nfrom greykite.framework.templates.autogen.forecast_config import EvaluationPeriodParam\nfrom greykite.framework.templates.autogen.forecast_config import ForecastConfig\nfrom greykite.framework.templates.autogen.forecast_config import MetadataParam\nfrom greykite.framework.templates.autogen.forecast_config import ModelComponentsParam\n\n# Evaluation metrics used in the tests.\n# F1 score for the True label:\nf1 = partial_return(f1_score, True)\n# Precision score, for the True label:\nprecision = partial_return(precision_score, True)\n# Recall score for the True label:\nrecall = partial_return(recall_score, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate a dataset with anomalies\nLet us first generate a dataset with ground truth anomaly labels.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df = generate_df_for_tests(\n    freq=\"D\",\n    train_start_date=datetime.datetime(2020, 1, 1),\n    intercept=50,\n    train_frac=0.99,\n    periods=200)[\"df\"]\n\n# Specifies anomaly locations.\nanomaly_block_list = [\n    np.arange(10, 15),\n    np.arange(33, 35),\n    np.arange(60, 65),\n    np.arange(82, 85),\n    np.arange(94, 98),\n    np.arange(100, 105),\n    np.arange(111, 113),\n    np.arange(125, 130),\n    np.arange(160, 163),\n    np.arange(185, 190),\n    np.arange(198, 200)]\n\n# Contaminates `df` with anomalies at the specified locations,\n# via `anomaly_block_list`.\n# If original value is y, the anomalous value is: (1 +/- delta)*y.\ndf = contaminate_df_with_anomalies(\n    df=df,\n    anomaly_block_list=anomaly_block_list,\n    delta_range_lower=0.25,\n    delta_range_upper=0.5,\n    value_col=VALUE_COL,\n    min_admissible_value=None,\n    max_admissible_value=None)\n\nfig = plot_lines_markers(\n    df=df,\n    x_col=TIME_COL,\n    line_cols=[\"contaminated_y\", \"y\"],\n    line_colors=[\"red\", \"blue\"],\n    title=\"Generation of daily anomalous data\")\nfig.update_yaxes()\nplotly.io.show(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The anomalies are generated by adding a random delta to the original value.\nThe plot above shows the original data (``y``) in blue and the contaminated data\n(``contaminated_y``) in red. We will drop the original data (``y``) and use the\ncontaminated data (``contaminated_y``) as the input to the anomaly detector.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df = df.drop(columns=[VALUE_COL]).rename(\n    columns={\"contaminated_y\": VALUE_COL})\ndf[ANOMALY_COL] = (df[ANOMALY_COL] == 1)\n\ntrain_size = int(100)\ndf_train = df[:train_size].reset_index(drop=True)\ndf_test = df[train_size:].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Structure of a Greykite AD model\nThe Greykite AD takes a ``forecast_config`` and ``ADConfig``\nand builds a detector which uses the forecast as baseline.\nThe fit consists of following stages:\n  - Fit a forecast model using the given ``forecast_config``.\n  - Fit a volatility model using the given ``ADConfig``.\n    This builds a `~greykite.algo.uncertainty.conditional.conf_interval.conf_interval`\n   model that optimizes over the parameters specified in the ``ADConfig``.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Any of the available forecast model\ntemplates (see :doc:`/pages/stepbystep/0100_choose_model`) work in conjunction\nwith the Greykite AD. In this example, we choose the \"SILVERKITE_EMPTY\" template.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "metadata = MetadataParam(\n    time_col=TIME_COL,\n    value_col=VALUE_COL,\n    train_end_date=None,\n    anomaly_info=None)\n\nevaluation_period = EvaluationPeriodParam(\n    test_horizon=0,\n    cv_max_splits=0)\n\nmodel_components = ModelComponentsParam(\n    autoregression={\n        \"autoreg_dict\": {\n            \"lag_dict\": {\"orders\": [7]},\n            \"agg_lag_dict\": None}},\n    events={\n        \"auto_holiday\": False,\n        \"holiday_lookup_countries\": [\"US\"],\n        \"holiday_pre_num_days\": 2,\n        \"holiday_post_num_days\": 2,\n        \"daily_event_df_dict\": None},\n    custom={\n        \"extra_pred_cols\": [\"dow\"],\n        \"min_admissible_value\": 0,\n        \"normalize_method\": \"zero_to_one\"})\n\nforecast_config = ForecastConfig(\n    model_template=\"SILVERKITE_EMPTY\",\n    metadata_param=metadata,\n    coverage=None,\n    evaluation_period_param=evaluation_period,\n    forecast_horizon=1,\n    model_components_param=model_components)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Greykite AD algorithm works with or without anomaly labels for training.\nThe reward function for the AD algorithm is updated accordingly.\nWhen no anomaly labels are provided, the AD algorithm uses ``target_anomaly_percent`` to determine\nthe anomaly score threshold. If anomaly labels are provided, the AD algorithm uses\n``precision``, ``recall`` or ``f1`` to determine the anomaly score threshold.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Anomaly labels are available\nLet us first consider the case where anomaly labels are available for training.\nYou can pass the anomaly labels in a few different ways:\n  - As the ``ANOMALY_COL`` column in the training dataframe (``train_data.df``).\n  - As a vector of anomaly labels in the training data (``train_data.y_true``).\n  - As a separate dataframe in the training data (``train_data.anomaly_df``).\n  - As a separate dataframe in the ``metadata_param`` in the ``forecast_config``.\nThe detector combines the anomaly labels from all these sources and stores it\nunder the ``anomaly_df`` attribute in the ``detector``.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, the anomaly labels are passed as ``ANOMALY_COL`` column in the training dataframe.\nWhen anomalies are available for training, you can use ``precision``, ``recall``,  ``f1`` or a combination\nof these metrics to determine the anomaly score threshold. In this example, we will use ``f1``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ad_config = ADConfig(\n    volatility_features_list=[[\"dow\"], [\"is_weekend\"]],\n    coverage_grid=[0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.9, 0.95, 0.99, 0.999],\n    variance_scaling=True)\n\ndef f1_reward(data):\n    return f1(\n        y_true=data.y_true,\n        y_pred=data.y_pred)\nreward = Reward(f1_reward)\ntrain_data = DetectorData(df=df_train)\n\n# Initializes the detector.\ndetector = GreykiteDetector(\n    forecast_config=forecast_config,\n    ad_config=ad_config,\n    reward=reward)\n# Fits the model\ndetector.fit(data=train_data)\n\n# Checks parameter grid.\nparam_obj_list = detector.fit_info[\"param_obj_list\"]\nparam_eval_df = pd.DataFrame.from_records(param_obj_list)\nparam_eval_df[\"volatility_features\"] = param_eval_df[\"volatility_features\"].map(str)\nfig = px.line(\n    param_eval_df,\n    x=\"coverage\",\n    y=\"obj_value\",\n    color=\"volatility_features\",\n    title=\"'GreykiteDetector' result of parameter search: reward=f1\")\nplotly.io.show(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plots the training results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = detector.plot(title=\"'GreykiteDetector' prediction: reward=f1\", phase=\"train\")\nplotly.io.show(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us run the model on the test data and plot the results.\nThe plot shows the actual data in orange, the forecast in blue, and the\nconfidence intervals in grey. The predicted anomalies are marked in red.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "test_data = DetectorData(\n    df=df_test,\n    y_true=df_test[ANOMALY_COL])\ntest_data = detector.predict(test_data)\nfig = detector.plot(title=\"'GreykiteDetector' prediction: reward=f1\")\nplotly.io.show(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see from the plot that our model is able to detect all the anomalies.\nFinally, let's check the evaluation metrics via the ``summary`` method.\nYou can see that the model achieved a high precision and recall value.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "summary = detector.summary()\nprint(summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Examples of other reward functions\nIn this section we provide examples of other reward functions that can be used.\nThe `~greykite.detection.detector.reward.Reward` class allows users the\nflexibility to specify their own reward functions. This class enables two powerful mechanisms:\n - taking a simple `reward_func` and construct a penalized version of that\n - starting from existing objectives building more complex ones by adding /\n multiplying / dividing them or use same operations with numbers.\n\nThese two mechanisms together support robust multi-objective problems.\nSome examples are provided below. All these reward functions can be used as before.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Builds precision as objective function.\ndef precision_func(data):\n    return precision(\n        y_true=data.y_true,\n        y_pred=data.y_pred)\nprecision_obj = Reward(precision_func)\n\n# Builds recall as objective function.\ndef recall_func(data):\n    return recall(\n        y_true=data.y_true,\n        y_pred=data.y_pred)\nrecall_obj = Reward(recall_func)\n\n# Builds sum of precision and recall objective function.\nadditive_obj = precision_obj + recall_obj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The class also allows for constrained optimization. For example, in the context\nof anomaly detection if recall is to be optimized\nsubject to precision being at least 80 percent, the users can enable this. Let's\nsee how this can be done.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# First, let's build a penalized precision objective function that\n# penalizes precision values under 0.8 by `penalty == -inf`.\npenalized_precision_obj = Reward(\n    precision_func,\n    min_unpenalized=0.8,\n    penalty=-np.inf)\n\n# The constraint can also be passed via the ADConfig.\nad_config = ADConfig(\n    target_precision=0.8)\n\n# Builds a combined objective function that optimizes recall\n# subject to precision being at least 80 percent.\ncombined_obj = recall_obj + penalized_precision_obj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Users can also combine objectives to achieve more complex objectives from existing ones.\nFor example F1 can be easily expressed in terms of precision and recall objectives.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "f1_obj = (2 * recall_obj * precision_obj) / (recall_obj + precision_obj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Anomaly labels are *NOT* available\nIn this example, we will use an AD config which uses ``target_anomaly_percent`` to\ndetermine the anomaly score threshold. If not specified, the AD algorithm uses a default\n``target_anomaly_percent`` of 10%.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ad_config = ADConfig(\n    volatility_features_list=[[\"dow\"], [\"is_weekend\"]],\n    coverage_grid=[0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.9, 0.95, 0.99, 0.999],\n    target_anomaly_percent=10.0,\n    variance_scaling=True)\n\ndetector = GreykiteDetector(\n    forecast_config=forecast_config,\n    ad_config=ad_config,\n    reward=None)\ndetector.fit(data=train_data)\n\n# Checks parameter grid.\nparam_obj_list = detector.fit_info[\"param_obj_list\"]\nparam_eval_df = pd.DataFrame.from_records(param_obj_list)\nparam_eval_df[\"volatility_features\"] = param_eval_df[\"volatility_features\"].map(str)\nfig = px.line(\n    param_eval_df,\n    x=\"coverage\",\n    y=\"obj_value\",\n    color=\"volatility_features\",\n    title=\"'GreykiteDetector' result of param search: reward=anomaly_percent\")\nplotly.io.show(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plots the training results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = detector.plot(title=\"'GreykiteDetector' prediction: reward=anomaly_percent\", phase=\"train\")\nplotly.io.show(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us run the model on the test data and plot the results.\nThe plot shows the actual data in orange, the forecast in blue, and the\nconfidence intervals in grey. The predicted anomalies are marked in red.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "test_data = DetectorData(\n    df=df_test,\n    y_true=df_test[ANOMALY_COL])\ntest_data = detector.predict(test_data)\nfig = detector.plot(title=\"'GreykiteDetector' prediction: reward=anomaly_percent\")\nplotly.io.show(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see from the plot that our model is able to detect all the anomalies.\nFinally, let's check the evaluation metrics via the ``summary`` method.\nYou can see that the model achieved a high precision and recall value.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "summary = detector.summary()\nprint(summary)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}