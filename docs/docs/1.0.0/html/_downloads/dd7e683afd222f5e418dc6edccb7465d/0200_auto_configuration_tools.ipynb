{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Auto Configuration Tools\n\nThe Silverkite model has many hyperparameters to tune.\nBesides domain knowledge, we also have tools that can help\nfind good choices for certain hyperparameters.\nIn this tutorial, we will present\n\n  * seasonality inferrer\n  * holiday inferrer\n  * holiday grouper\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>If you use the model templates, you can specify the \"auto\" option for certain model components\n  (growth, seasonality and holiday),\n  and the auto configuration tool will be activated automatically.\n  See `auto seasonality <../../../pages/model_components/0300_seasonality.html#silverkite>`_,\n  `auto growth <../../../pages/model_components/0500_changepoints.html#auto-growth>`_ and\n  `auto holidays <../../../pages/model_components/0400_events.html#auto-holiday>`_ for the way to activate them.\n  This doc explains how the \"auto\" options work behind the code.\n  You can replay the \"auto\" options with the Seasonality Inferrer and Holiday Inferrer below.\n  Please remember that if you are doing train-test split,\n  running the inferrers on training data only is closer to the reality.</p></div>\n\n## Seasonality Inferrer\n\nThe Silverkite model uses Fourier series to model seasonalities.\nIt's sometimes difficult to decide what orders we should use\nfor each Fourier series.\nLarger orders tend to fit more closely to the curves, while having\nthe risk of overfitting.\nSmall orders tend to underfit the curve and may not learn the exact seasonality patterns.\n\n`~greykite.algo.common.seasonality_inferrer.SeasonalityInferrer`\nis a tool that can help you decide what order to use for a seasonality's Fourier series.\nNote that there are many ways to decide the orders,\nand you don't have to strictly stick to the results from Seasonality Inferrer.\n\n### How it works\n\nThe seasonality inferrer utilizes criteria including AIC and BIC to find the most\nappropriate Fourier series orders.\nFor a specific seasonality, e.g. yearly seasonality, the steps are as follows:\n\n* Trend removal: seasonality inferrer provides 4 options for trend removal.\n  They are listed in `~greykite.algo.common.seasonality_inferrer.TrendAdjustMethodEnum`.\n  Specifically:\n\n    * ``\"seasonal_average\"``: given an indicator of seasonal period, the method subtracts\n      the average within each seasonal period from the original time series.\n      For example, given the column ``year``, the average is calculated on each different year.\n    * ``\"overall_average\"``: subtracts the overall average from the original time series.\n    * ``\"spline_fit\"``: fits a polynomial up to a given degree and subtract from the original time series.\n    * ``\"none\"``: does not adjust the trend.\n\n  Typically \"seasonal_average\" is a good choice with appropriate columns.\n  For example, we can use ``year_quarter`` for quarterly seasonality, ``year_month`` for monthly seasonality,\n  ``year_woy_iso`` for weekly seasonality and ``year_woy_dow_iso`` for daily seasonality.\n* Optional aggregation: sometimes we want to get rid of shorter fluctuations before\n  fitting a longer seasonality period. We can do an optional aggregation beforehand.\n  For example, when we model yearly seasonality, we can do a ``\"7D\"`` aggregation to eliminate\n  weekly effects to make the result more stable.\n* With a pre-specified maximum order ``n``, we fit the de-trended (and aggregated) time series\n  with Fourier series from 1 to n, and calculate the AIC/BIC for those fits.\n  The most appropriate order is then decided by choosing the order with best AIC or BIC.\n  The method also allows to slightly sacrifice the criterion and reduce the order\n  for less risk of overfitting using the ``tolerance`` parameter.\n* Finally, an optional offset can be applied to any inferred orders to allow manual adjustments.\n  For example, if one would like to use less yearly seasonality order, they may specify\n  offset for yearly seasonality to be -2, and the final order will subtract 2 from the inferred result.\n  This is useful when users tend to use more or less orders to model seasonality,\n  and want a knob on top of the inferring results.\n\n### Example\n\nNow we look at an example with the Peyton-Manning Wiki page view data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\nimport plotly\nfrom greykite.common.data_loader import DataLoader\nfrom greykite.algo.common.seasonality_inferrer import SeasonalityInferConfig\nfrom greykite.algo.common.seasonality_inferrer import SeasonalityInferrer\nfrom greykite.algo.common.seasonality_inferrer import TrendAdjustMethodEnum\nfrom greykite.common import constants as cst"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ``SeasonalityInferrer`` class uses\n`~greykite.algo.common.seasonality_inferrer.SeasonalityInferConfig`\nto specify configuration for a single seasonality component,\nand it takes a list of such configurations to infer multiple seasonality\ncomponents together.\nNow we specify seasonality inferring configs for yearly to weekly seasonalities.\nIn each of these configs, specify the parameters that are distinct for each component.\nIf there are parameters that are the same across all configs,\nyou can specify them in the function directly.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "yearly_config = SeasonalityInferConfig(\n    seas_name=\"yearly\",                     # name for seasonality\n    col_name=\"toy\",                         # column to generate Fourier series, fixed for yearly\n    period=1.0,                             # seasonal period, fixed for yearly\n    max_order=30,                           # max number of orders to model\n    adjust_trend_param=dict(\n        trend_average_col=\"year\"\n    ),                                      # column to adjust trend for method \"seasonal_average\"\n    aggregation_period=\"W\",                 # aggregation period,\n    offset=0                                # add this to the inferred result, default 0\n)\nquarterly_config = SeasonalityInferConfig(\n    seas_name=\"quarterly\",                  # name for seasonality\n    col_name=\"toq\",                         # column to generate Fourier series, fixed for quarterly\n    period=1.0,                             # seasonal period, fixed for quarterly\n    max_order=20,                           # max number of orders to model\n    adjust_trend_param=dict(\n        trend_average_col=\"year_quarter\"\n    ),                                      # column to adjust trend for method \"seasonal_average\"\n    aggregation_period=\"2D\",                # aggregation period\n)\nmonthly_config = SeasonalityInferConfig(\n    seas_name=\"monthly\",                    # name for seasonality\n    col_name=\"tom\",                         # column to generate Fourier series, fixed for monthly\n    period=1.0,                             # seasonal period, fixed for monthly\n    max_order=20,                           # max number of orders to model\n    adjust_trend_param=dict(\n        trend_average_col=\"year_month\"\n    ),                                      # column to adjust trend for method \"seasonal_average\"\n    aggregation_period=\"D\"                  # aggregation period\n)\nweekly_config = SeasonalityInferConfig(\n    seas_name=\"weekly\",                     # name for seasonality\n    col_name=\"tow\",                         # column to generate Fourier series, fixed for weekly\n    period=7.0,                             # seasonal period, fixed for weekly\n    max_order=10,                           # max number of orders to model\n    adjust_trend_param=dict(\n        trend_average_col=\"year_woy_iso\"\n    ),                                      # column to adjust trend for method \"seasonal_average\"\n    aggregation_period=\"D\",\n    tolerance=0.005,                        # allows 0.5% higher criterion for lower orders\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we put everything together to infer seasonality effects.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df = DataLoader().load_peyton_manning()\ndf[cst.TIME_COL] = pd.to_datetime((df[cst.TIME_COL]))\n\nmodel = SeasonalityInferrer()\nresult = model.infer_fourier_series_order(\n    df=df,\n    time_col=cst.TIME_COL,\n    value_col=cst.VALUE_COL,\n    configs=[\n        yearly_config,\n        quarterly_config,\n        monthly_config,\n        weekly_config\n    ],\n    adjust_trend_method=TrendAdjustMethodEnum.seasonal_average.name,\n    fit_algorithm=\"linear\",\n    plotting=True,\n    criterion=\"bic\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The method runs quickly and we can simply extract the inferred results\nfrom the output.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result[\"best_orders\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also plot the results to see how different orders vary the criterion.\nSimilar to other trade-off plots, the plot first goes down and then goes up,\nreaching the best at some appropriate value in the middle.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# The [0] extracts the first seasonality component from the results.\nplotly.io.show(result[\"result\"][0][\"fig\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Holiday Inferrer\n\nThe Silverkite model supports modeling holidays and their neighboring days\nas indicators. Significant days are modeled separately,\nwhile similar days can be grouped together as one indicator,\nassuming their effects are the same.\n\nIt's sometimes difficult to decide which holidays to include,\nto model separately or to model together.\n`~greykite.algo.common.holiday_inferrer.HolidayInferrer`\nis a tool that can help you decide which holidays to model\nand how to model them.\nIt can also automatically generate the holiday configuration parameters.\nNote that there are many ways to decide the holiday configurations,\nand you don't have to strictly stick to the results from Holiday Inferrer.\n\n### How it works\n\nThe holiday inferrer estimates individual holiday or their\nneighboring days' effects by comparing the observations\non these days with some baseline prior to or after the holiday period.\nThen it ranks the effects by their magnitude.\nDepending on some thresholds, it decides whether to model\na day independently, together with others or do not model it.\n\nIn detail, the first step is to unify the data frequency.\nFor data whose frequency is greater than daily,\nholiday effect is automatically turned off.\nFor data whose frequency is less than daily,\nit is aggregated into daily data,\nsince holidays are daily events.\nFrom now on, we have daily data for the next step.\n\nGiven a list of countries, the tool automatically pulls candidate\nholidays from the database. With a ``pre_search_days`` and a ``post_search_days``\nparameters, those holidays' neighboring days are included in the candidate pool\nas well.\n\nFor every candidate holiday or neighboring day,\nthe baseline is the average of a configurable offsets.\nFor example, for data that exhibits strong weekly seasonality,\nthe offsets can be ``(-7, 7)``, where the baseline will be\nthe average of the last same day of week's observation and the\nnext same day of week's observation.\nFor example, if the holiday is New Year on 1/1 while 12/25 (7 days ago) is Christmas,\nit will look at the value on 12/18 instead of 12/25 as baseline.\n\nThe day's effect is the average of the signed difference between\nthe true observation and the baseline across all occurrences in the time series.\nThe effects are ranked from the highest to the lowest by their absolute effects.\n\nTo decide how each holiday is modeled, we rely on two parameters:\n``independent_holiday_thres`` and ``together_holiday_thres``.\nThese parameters are between 0 and 1.\nStarting from the largest effect,\nwe calculate the cumulative sum of effect of all candidates.\nOnce the cumulative effect reaches ``independend_holiday_thres`` of the total effects,\nthese days will be modeled independently (i.e, each day has an individual coefficient).\nWe keep accumulating effects until the sum reaches ``together_holiday_thres``,\nthe days in the between are grouped into \"positive_group\" and \"negative_group\",\nwith each group modeled together.\n\n### Example\n\nNow we look at an example with the Peyton-Manning Wiki page view data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\nimport plotly\nfrom greykite.algo.common.holiday_inferrer import HolidayInferrer\nfrom greykite.common.data_loader import DataLoader\nfrom greykite.common import constants as cst\n\ndf = DataLoader().load_peyton_manning()\ndf[cst.TIME_COL] = pd.to_datetime(df[cst.TIME_COL])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's say we want to infer the holidays in the United States,\nwith consideration on +/- 2 days of each holiday as potential candidates too.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "hi = HolidayInferrer()\nresult = hi.infer_holidays(\n    df=df,\n    countries=[\"US\"],                   # Search holidays in United States\n    plot=True,                          # Output a plot\n    pre_search_days=2,                  # Considers 2 days before each holiday\n    post_search_days=2,                 # Considers 2 days after each holiday\n    independent_holiday_thres=0.9,      # The first 90% of effects are modeled separately\n    together_holiday_thres=0.99,        # The 90% to 99% of effects are modeled together\n    baseline_offsets=[-7, 7]            # The baseline is the average of -7/+7 observations\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can plot the inferred holiday results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plotly.io.show(result[\"fig\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The class also has a method to generate the holiday configuration\nbased on the inferred results, that is consumable directly by the Silverkite model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "hi.generate_daily_event_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Holiday Grouper\n\nOne step further, `~greykite.algo.common.holiday_grouper.HolidayGrouper`\nis a convenient tool that automatically groups similar holidays and their neighboring days\ntogether based on their estimated impact and clustering algorithms.\nThis helps to (1) reduce the number of parameters to be estimated\nand have each group have sufficient data points to be reliably estimated;\n(2) make sure different holidays can be separately modeled to avoid confounding effects.\n\nAlso, we provide flexible diagnostics to help users choose the number of groups, as well as\nutility functions to spot check which group a holiday belongs to and what are the similar\nholidays within the same group.\n\n### How it works\n\nFirst, we need to supply the algorithm a list of holidays and dates, as well as a time series of interest.\nIn addition, we specify a dictionary of neighboring days that a holiday may have effect on.\nFor example, for Thanksgiving that always falls on Thursday, we may expect a holiday effect\nthat starts the day before and lasts till the coming Monday, then we can specify\n``\"Thanksgiving\": (1, 4)`` as an item in the dictionary.\nAll the neighboring days specified as such will be added to the events pool.\nNote that each neighboring day is also treated as a single event, and may not end up with the same group\nas its original holiday date.\nThat is, ``\"Thanksgiving_plus_4\"`` (Monday) may have a very different impact than\n``\"Thanksgiving`` (Thursday) and they may not end up with being in the same group.\n\nSecond, we also note that holidays falling on weekdays may have a different impact than those on weekends.\nFor example, ``\"Christmas Day_WE\"`` may have a different effect than ``\"Christmas Day_WD\"``.\nWe included two built-in options (\"wd_we\": weekday vs weekend, \"dow_grouped\": weekday, Sat, Sun), but one\ncan custom their own grouping via ``get_suffix_func`` parameter.\n\nNext, each single event gets a score, the estimated (relative) impact that uses the same methodology\nas in the Holiday Inferrer (e.g. -0.1 means 10% lower than the baseline).\nFor example, you can use ``baseline_offsets=[-7, 7]``.\nThe score will then be used for the clustering algorithm. Therefore, if an event only shows up once\nin the input time series, the estimated impact may not be accurate.\nOne can set the minimal number of occurrences of an event by parameter ``min_n_days`` (set it to 1 if\nyou are okay with including all events that appear only once on a single day in the input data).\nAlso, you can specify the minimal average score of an event to be kept in consideration by ``min_abs_avg_score``.\nIf an event has an average score of -1% (across all its occurrences), it may not be worth including in the model.\nAbsolute effects lower than ``min_abs_avg_score`` will be excluded before clustering.\nAlso, if an event have inconsistent scores (e.g. two occurrences have -8%, +5% respectively), then this could be\nnoise rather than signal. These events are excluded as well.\nThis is handled automatically and user does not need to worry about it.\n\nThe last step of the grouper is to group events that have similar effects and generate ``daily_event_df_dict``.\nWe provide two options for clustering, Kernel Density Estimation (``clustering_method=\"kde\"``)\nand K-means (``clustering_method=\"kmeans\"``).\nIn K-means, you can specify ``n_clusters`` to your desired number of groups.\nIn KDE clustering, you can change the default bandwidth parameter to adjust the number of groups you get.\nDepending on the length of the time series and the number of holidays considered, we recommend a range from 5 to\n15 groups. You can check the visualization / diagnostics via attribute ``self.result_dict[\"kmean_plot\"]``\nor ``self.result_dict[\"kde_plot\"]``, respectively.\nSee `~greykite.algo.common.holiday_grouper.HolidayGrouper.group_holidays` for more parameter details.\n\n### Example\n\nNow we look at an example with the Peyton-Manning Wiki page view data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\nimport plotly\nfrom greykite.algo.common.holiday_grouper import HolidayGrouper\nfrom greykite.common.data_loader import DataLoader\nfrom greykite.common.features.timeseries_features import get_holidays\nfrom greykite.common import constants as cst\n\ndf = DataLoader().load_peyton_manning()\ndf[cst.TIME_COL] = pd.to_datetime(df[cst.TIME_COL])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's generate a list of holidays in the United States, and we\nalso specify the neighboring days we want to consider in the holiday model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "year_start = df[cst.TIME_COL].dt.year.min() - 1\nyear_end = df[cst.TIME_COL].dt.year.max() + 1\nholiday_df = get_holidays(countries=[\"US\"], year_start=year_start, year_end=year_end)[\"US\"]\n\n# Defines the number of pre / post days that a holiday has impact on.\n# If not specified, numbers specified by ``holiday_impact_pre_num_days`` and\n# ``holiday_impact_post_num_days`` will be used.\nholiday_impact_dict = {\n    \"Christmas Day\": (4, 3),  # 12/25.\n    \"Independence Day\": (4, 4),  # 7/4.\n    \"Juneteenth National Independence Day\": (3, 3),  # 6/19.\n    \"Labor Day\": (3, 1),  # Monday.\n    \"Martin Luther King Jr. Day\": (3, 1),  # Monday.\n    \"Memorial Day\": (3, 1),  # Monday.\n    \"New Year's Day\": (3, 4),  # 1/1.\n    \"Thanksgiving\": (1, 4),  # Thursday.\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we run the holiday grouper with K-means clustering.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Instantiates `HolidayGrouper`.\nhg = HolidayGrouper(\n    df=df,\n    time_col=cst.TIME_COL,\n    value_col=cst.VALUE_COL,\n    holiday_df=holiday_df,\n    holiday_date_col=\"date\",\n    holiday_name_col=\"event_name\",\n    holiday_impact_pre_num_days=0,\n    holiday_impact_post_num_days=0,\n    holiday_impact_dict=holiday_impact_dict,\n    get_suffix_func=\"wd_we\"\n)\n\n# Runs holiday grouper using k-means with diagnostics.\nhg.group_holidays(\n    baseline_offsets=[-7, 7],\n    min_n_days=2,\n    min_abs_avg_score=0.03,\n    clustering_method=\"kmeans\",\n    n_clusters=6,\n    include_diagnostics=True\n)\n\nresult_dict = hg.result_dict\ndaily_event_df_dict = result_dict[\"daily_event_df_dict\"]  # Can be directed used in events."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check results. For example, we can check the score and grouping of New Year's Day that falls on weekdays.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "hg.check_scores(\"New Year's Day_WD\")\nhg.check_holiday_group(\"New Year's Day_WD\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check the diagnostics plot for K-means clustering.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plotly.io.show(result_dict[\"kmeans_plot\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's try clustering using KDE and check the results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "hg.group_holidays(\n    baseline_offsets=[-7, 7],\n    min_n_days=1,\n    min_abs_avg_score=0.03,\n    bandwidth_multiplier=0.5,\n    clustering_method=\"kde\"\n)\nresult_dict = hg.result_dict\ndaily_event_df_dict = result_dict[\"daily_event_df_dict\"]\n\nplotly.io.show(result_dict[\"kde_plot\"])\n# Checks the number of events in each group.\nfor event_group, event_df in daily_event_df_dict.items():\n    print(f\"{event_group}: contains {event_df.shape[0]} days.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}