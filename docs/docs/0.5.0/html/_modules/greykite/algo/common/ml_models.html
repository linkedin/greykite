<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>greykite.algo.common.ml_models &mdash; Greykite Library  documentation</title>
      <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/sg_gallery.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/sg_gallery-binder.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/sg_gallery-dataframe.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/sphinx_highlight.js"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../index.html" class="icon icon-home"> Greykite Library
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Greykite Info</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../pages/greykite/overview.html">Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../gallery/quickstart/index.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../gallery/tutorials/index.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../gallery/templates/index.html">Model Templates</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Step by Step</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../pages/stepbystep/0000_stepbystep.html">Forecasting Process</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pages/stepbystep/0100_choose_model.html">Choose a Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pages/stepbystep/0200_choose_template.html">Choose a Model Template</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pages/stepbystep/0300_input.html">Examine Input Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pages/stepbystep/0400_configuration.html">Configure a Forecast</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pages/stepbystep/0500_output.html">Check Forecast Result</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pages/stepbystep/0600_debug.html">Debugging</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tuning the Model Components</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../pages/model_components/0100_introduction.html">Greykite models and components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pages/model_components/0200_growth.html">Growth</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pages/model_components/0300_seasonality.html">Seasonality</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pages/model_components/0400_events.html">Holidays and Events</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pages/model_components/0500_changepoints.html">Changepoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pages/model_components/0600_custom.html">Custom Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pages/model_components/0700_regressors.html">Regressors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pages/model_components/0800_autoregression.html">Auto-regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pages/model_components/0900_uncertainty.html">Uncertainty Intervals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pages/model_components/1000_override.html">Pre-processing, Selective Grid Search</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Benchmarking</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../pages/benchmarking/benchmarking.html">Benchmarking</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Miscellaneous</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../pages/miscellaneous/reconcile_forecasts.html">Reconcile Forecasts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pages/miscellaneous/store_model.html">Model store and load</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Changelog</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../pages/changelog/changelog.html">0.5.0 (2023-04-03)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pages/changelog/changelog.html#id2">0.4.0 (2022-07-15)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pages/changelog/changelog.html#id3">0.3.0 (2021-12-14)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pages/changelog/changelog.html#id4">0.2.0 (2021-06-30)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pages/changelog/changelog.html#id5">0.1.1 (2021-05-12)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../pages/autodoc/doc.html">Docs</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">Greykite Library</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home"></a></li>
          <li class="breadcrumb-item"><a href="../../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">greykite.algo.common.ml_models</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for greykite.algo.common.ml_models</h1><div class="highlight"><pre>
<span></span><span class="c1"># BSD 2-CLAUSE LICENSE</span>

<span class="c1"># Redistribution and use in source and binary forms, with or without modification,</span>
<span class="c1"># are permitted provided that the following conditions are met:</span>

<span class="c1"># Redistributions of source code must retain the above copyright notice, this</span>
<span class="c1"># list of conditions and the following disclaimer.</span>
<span class="c1"># Redistributions in binary form must reproduce the above copyright notice,</span>
<span class="c1"># this list of conditions and the following disclaimer in the documentation</span>
<span class="c1"># and/or other materials provided with the distribution.</span>
<span class="c1"># THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS &quot;AS IS&quot; AND</span>
<span class="c1"># ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED</span>
<span class="c1"># WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE</span>
<span class="c1"># DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR</span>
<span class="c1"># #ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES</span>
<span class="c1"># (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;</span>
<span class="c1"># LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND</span>
<span class="c1"># ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT</span>
<span class="c1"># (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS</span>
<span class="c1"># SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</span>
<span class="c1"># original author: Reza Hosseini, Yi Su</span>
<span class="sd">&quot;&quot;&quot;Functions to fit a machine learning model</span>
<span class="sd">and use it for prediction.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">traceback</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>

<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">patsy</span>
<span class="kn">import</span> <span class="nn">scipy</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
<span class="kn">from</span> <span class="nn">pandas.plotting</span> <span class="kn">import</span> <span class="n">register_matplotlib_converters</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">ElasticNetCV</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LarsCV</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LassoCV</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LassoLarsCV</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">RidgeCV</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDRegressor</span>

<span class="kn">from</span> <span class="nn">greykite.algo.common.l1_quantile_regression</span> <span class="kn">import</span> <span class="n">QuantileRegression</span>
<span class="kn">from</span> <span class="nn">greykite.algo.uncertainty.conditional.conf_interval</span> <span class="kn">import</span> <span class="n">conf_interval</span>
<span class="kn">from</span> <span class="nn">greykite.algo.uncertainty.conditional.conf_interval</span> <span class="kn">import</span> <span class="n">predict_ci</span>
<span class="kn">from</span> <span class="nn">greykite.common.constants</span> <span class="kn">import</span> <span class="n">RESIDUAL_COL</span>
<span class="kn">from</span> <span class="nn">greykite.common.constants</span> <span class="kn">import</span> <span class="n">R2_null_model_score</span>
<span class="kn">from</span> <span class="nn">greykite.common.evaluation</span> <span class="kn">import</span> <span class="n">calc_pred_err</span>
<span class="kn">from</span> <span class="nn">greykite.common.evaluation</span> <span class="kn">import</span> <span class="n">r2_null_model_score</span>
<span class="kn">from</span> <span class="nn">greykite.common.features.normalize</span> <span class="kn">import</span> <span class="n">normalize_df</span>
<span class="kn">from</span> <span class="nn">greykite.common.logging</span> <span class="kn">import</span> <span class="n">LoggingLevelEnum</span>
<span class="kn">from</span> <span class="nn">greykite.common.logging</span> <span class="kn">import</span> <span class="n">log_message</span>
<span class="kn">from</span> <span class="nn">greykite.common.python_utils</span> <span class="kn">import</span> <span class="n">group_strs_with_regex_patterns</span>
<span class="kn">from</span> <span class="nn">greykite.common.viz.timeseries_plotting</span> <span class="kn">import</span> <span class="n">plot_multivariate</span>


<span class="n">matplotlib</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">&quot;agg&quot;</span><span class="p">)</span>  <span class="c1"># noqa: E402</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>  <span class="c1"># isort:skip # noqa: E402</span>


<span class="n">register_matplotlib_converters</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">get_intercept_col_from_design_mat</span><span class="p">(</span>
        <span class="n">x_mat</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Gets the explicit or implicit intercept column name from `patsy` design matrix.</span>

<span class="sd">    By default, `patsy` will make the design matrix always full rank.</span>
<span class="sd">    It will always include an intercept term unless we specify &quot;-1&quot; or &quot;+0&quot;.</span>
<span class="sd">    However, if there are categorical variables, even we specify &quot;-1&quot; or &quot;+0&quot;,</span>
<span class="sd">    it will include an implicit intercept by adding all levels of a categorical</span>
<span class="sd">    variable into the design matrix.</span>

<span class="sd">    The logic in patsy is that when intercept is excluded,</span>
<span class="sd">    always the first categorical variable in the formula string will have all levels.</span>
<span class="sd">    The levels are ordered in alphabetical order.</span>
<span class="sd">    In this case, we will search for the first categorical variable</span>
<span class="sd">    and remove its first level.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x_mat : `pandas.DataFrame`</span>
<span class="sd">        The design matrix built by `patsy`.</span>
<span class="sd">        Must have attribute ``design_info``.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    name : `str` or None</span>
<span class="sd">        The column name of explicit or implicit intercept in ``x_mat``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">design_info</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">x_mat</span><span class="p">,</span> <span class="s2">&quot;design_info&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">name</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">design_info</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">terms</span> <span class="o">=</span> <span class="n">design_info</span><span class="o">.</span><span class="n">terms</span>
        <span class="c1"># Checks if intercept is in the design matrix.</span>
        <span class="k">if</span> <span class="n">patsy</span><span class="o">.</span><span class="n">desc</span><span class="o">.</span><span class="n">Term</span><span class="p">([])</span> <span class="ow">in</span> <span class="n">terms</span><span class="p">:</span>
            <span class="n">name</span> <span class="o">=</span> <span class="n">patsy</span><span class="o">.</span><span class="n">desc</span><span class="o">.</span><span class="n">Term</span><span class="p">([])</span><span class="o">.</span><span class="n">name</span><span class="p">()</span>  <span class="c1"># Name is &quot;Intercept&quot;.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Intercept is not in design matrix,</span>
            <span class="c1"># finds the implicit intercept.</span>
            <span class="c1"># `patsy` orders categorical variables first,</span>
            <span class="c1"># and the first categorical variable has all levels.</span>
            <span class="c1"># We remove the first level.</span>
            <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">idx_slice</span> <span class="ow">in</span> <span class="n">design_info</span><span class="o">.</span><span class="n">term_name_slices</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="c1"># We only need to iterate the first element.</span>
                <span class="k">if</span> <span class="n">idx_slice</span><span class="o">.</span><span class="n">stop</span> <span class="o">-</span> <span class="n">idx_slice</span><span class="o">.</span><span class="n">start</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="c1"># Sets name to the first column only when the term has</span>
                    <span class="c1"># more than 1 columns (ignores no-categorical case).</span>
                    <span class="n">name</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">x_mat</span><span class="o">.</span><span class="n">columns</span><span class="p">)[</span><span class="n">idx_slice</span><span class="o">.</span><span class="n">start</span><span class="p">]</span>
                <span class="k">break</span>
    <span class="k">return</span> <span class="n">name</span>


<span class="k">def</span> <span class="nf">design_mat_from_formula</span><span class="p">(</span>
        <span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
        <span class="n">model_formula_str</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">y_col</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">pred_cols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">remove_intercept</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Given a formula it extracts the response vector (y)</span>
<span class="sd">    and builds the design matrix (x_mat).</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    df : `pandas.DataFrame`</span>
<span class="sd">        A dataframe with the response vector (y) and the feature columns (x_mat).</span>
<span class="sd">    model_formula_str : `str`</span>
<span class="sd">        A formula string e.g. &quot;y~x1+x2+x3*x4&quot;.</span>
<span class="sd">        This is similar to R formulas.</span>
<span class="sd">        See https://patsy.readthedocs.io/en/latest/formulas.html#how-formulas-work.</span>
<span class="sd">    y_col : `str` or None, default None</span>
<span class="sd">        The column name which has the value of interest to be forecasted.</span>
<span class="sd">        If the model_formula_str is not passed, y_col e.g. [&quot;y&quot;] is used</span>
<span class="sd">        as the response vector column.</span>
<span class="sd">    pred_cols : `list` [`str`] or None, default None</span>
<span class="sd">        The names of the feature columns.</span>
<span class="sd">        If the model_formula_str is not passed, pred_cols e.g.</span>
<span class="sd">        [&quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;] is used as the design matrix columns.</span>
<span class="sd">    remove_intercept : `bool`, default False</span>
<span class="sd">        Whether to remove explicit and implicit intercepts.</span>
<span class="sd">        By default, `patsy` will make the design matrix always full rank.</span>
<span class="sd">        It will always include an intercept term unless we specify &quot;-1&quot; or &quot;+0&quot;.</span>
<span class="sd">        However, if there are categorical variables, even we specify &quot;-1&quot; or &quot;+0&quot;,</span>
<span class="sd">        it will include an implicit intercept by adding all levels of a categorical</span>
<span class="sd">        variable into the design matrix.</span>
<span class="sd">        Sometimes we don&#39;t want this to happen.</span>
<span class="sd">        Setting this parameter to True will remove both explicit and implicit intercepts.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    result : `dict`</span>
<span class="sd">        Result dictionary with the following keys:</span>

<span class="sd">            - &quot;y&quot;: The response vector.</span>
<span class="sd">            - &quot;y_col&quot;: Name of the response column (y).</span>
<span class="sd">            - &quot;x_mat&quot;: A design matrix.</span>
<span class="sd">            - &quot;pred_cols&quot;: Name of the columns of the design matrix (x_mat).</span>
<span class="sd">            - &quot;x_design_info&quot;: Information for design matrix.</span>
<span class="sd">            - &quot;drop_intercept_col&quot;: The intercept column to be dropped.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">intercept_col</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">model_formula_str</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">y</span><span class="p">,</span> <span class="n">x_mat</span> <span class="o">=</span> <span class="n">patsy</span><span class="o">.</span><span class="n">dmatrices</span><span class="p">(</span>
            <span class="n">model_formula_str</span><span class="p">,</span>
            <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span>
            <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;dataframe&quot;</span><span class="p">)</span>
        <span class="n">x_design_info</span> <span class="o">=</span> <span class="n">x_mat</span><span class="o">.</span><span class="n">design_info</span>
        <span class="k">if</span> <span class="n">remove_intercept</span><span class="p">:</span>
            <span class="n">intercept_col</span> <span class="o">=</span> <span class="n">get_intercept_col_from_design_mat</span><span class="p">(</span>
                <span class="n">x_mat</span><span class="o">=</span><span class="n">x_mat</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">intercept_col</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">x_mat</span> <span class="o">=</span> <span class="n">x_mat</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="n">intercept_col</span><span class="p">)</span>
        <span class="n">pred_cols</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">x_mat</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
        <span class="c1"># Gets the response column name using &quot;~&quot; location.</span>
        <span class="n">y_col</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="s2">&quot;(.*)~&quot;</span><span class="p">,</span> <span class="n">model_formula_str</span><span class="p">)</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">y</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
    <span class="k">elif</span> <span class="n">y_col</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">pred_cols</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">y_col</span><span class="p">]</span>
        <span class="n">x_mat</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">pred_cols</span><span class="p">]</span>
        <span class="n">x_design_info</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Either provide a model expression or both y_col and pred_cols.&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="n">y</span><span class="p">,</span>
        <span class="s2">&quot;y_col&quot;</span><span class="p">:</span> <span class="n">y_col</span><span class="p">,</span>
        <span class="s2">&quot;x_mat&quot;</span><span class="p">:</span> <span class="n">x_mat</span><span class="p">,</span>
        <span class="s2">&quot;pred_cols&quot;</span><span class="p">:</span> <span class="n">pred_cols</span><span class="p">,</span>
        <span class="s2">&quot;x_design_info&quot;</span><span class="p">:</span> <span class="n">x_design_info</span><span class="p">,</span>
        <span class="s2">&quot;drop_intercept_col&quot;</span><span class="p">:</span> <span class="n">intercept_col</span>
    <span class="p">}</span>


<span class="k">def</span> <span class="nf">fit_model_via_design_matrix</span><span class="p">(</span>
        <span class="n">x_train</span><span class="p">,</span>
        <span class="n">y_train</span><span class="p">,</span>
        <span class="n">fit_algorithm</span><span class="p">,</span>
        <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">fit_algorithm_params</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Fits the predictive model and returns the prediction function.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x_train : `numpy.array`</span>
<span class="sd">        The design matrix</span>
<span class="sd">    y_train : `numpy.array`</span>
<span class="sd">        The vector of responses</span>
<span class="sd">    fit_algorithm : `str`</span>
<span class="sd">        The type of predictive model used in fitting</span>
<span class="sd">        (implemented by sklearn and statsmodels).</span>

<span class="sd">        Available models are:</span>

<span class="sd">            - ``&quot;statsmodels_ols&quot;``   : `statsmodels.regression.linear_model.OLS`</span>
<span class="sd">            - ``&quot;statsmodels_wls&quot;``   : `statsmodels.regression.linear_model.WLS`</span>
<span class="sd">            - ``&quot;statsmodels_gls&quot;``   : `statsmodels.regression.linear_model.GLS`</span>
<span class="sd">            - ``&quot;statsmodels_glm&quot;``   : `statsmodels.genmod.generalized_linear_model.GLM`</span>
<span class="sd">            - ``&quot;linear&quot;``            : `statsmodels.regression.linear_model.OLS`</span>
<span class="sd">            - ``&quot;elastic_net&quot;``       : `sklearn.linear_model.ElasticNetCV`</span>
<span class="sd">            - ``&quot;ridge&quot;``             : `sklearn.linear_model.RidgeCV`</span>
<span class="sd">            - ``&quot;lasso&quot;``             : `sklearn.linear_model.LassoCV`</span>
<span class="sd">            - ``&quot;sgd&quot;``               : `sklearn.linear_model.SGDRegressor`</span>
<span class="sd">            - ``&quot;lars&quot;``              : `sklearn.linear_model.LarsCV`</span>
<span class="sd">            - ``&quot;lasso_lars&quot;``        : `sklearn.linear_model.LassoLarsCV`</span>
<span class="sd">            - ``&quot;rf&quot;``                : `sklearn.ensemble.RandomForestRegressor`</span>
<span class="sd">            - ``&quot;gradient_boosting&quot;`` : `sklearn.ensemble.GradientBoostingRegressor`</span>
<span class="sd">            - ``&quot;quantile_regression&quot;`` : `~greykite.algo.common.l1_quantile_regression.QuantileRegression`</span>

<span class="sd">        See `~greykite.algo.common.ml_models.fit_model_via_design_matrix`</span>
<span class="sd">        for the sklearn and statsmodels classes that implement these methods, and their parameters.</span>

<span class="sd">        &quot;linear&quot; is the same as &quot;statsmodels_ols&quot;, because `statsmodels.regression.linear_model.OLS`</span>
<span class="sd">        is more stable than `sklearn.linear_model.LinearRegression`.</span>
<span class="sd">    sample_weight : `numpy.array` or None, default None</span>
<span class="sd">        The vector of weights to be used in weighted models.</span>
<span class="sd">        These weights will be used to weigh each loss potentially differently.</span>
<span class="sd">    fit_algorithm_params : `dict` or None, default None</span>
<span class="sd">        Parameters passed to the requested ``fit_algorithm``.</span>
<span class="sd">        If None, uses the defaults defined in this function.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ml_model : `class`</span>
<span class="sd">        a trained predictive model with available predict method</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">fit_algorithm_dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;statsmodels_ols&quot;</span><span class="p">:</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">,</span>  <span class="c1"># ordinary least squares</span>
        <span class="s2">&quot;statsmodels_wls&quot;</span><span class="p">:</span> <span class="n">sm</span><span class="o">.</span><span class="n">WLS</span><span class="p">,</span>  <span class="c1"># weighted least squares</span>
        <span class="s2">&quot;statsmodels_gls&quot;</span><span class="p">:</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLS</span><span class="p">,</span>  <span class="c1"># generalized least squares</span>
        <span class="s2">&quot;statsmodels_glm&quot;</span><span class="p">:</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">,</span>  <span class="c1"># generalized linear models</span>
        <span class="c1"># &quot;linear&quot; has been redirected to statsmodels OLS instead of sklearn LinearRegression</span>
        <span class="c1"># We&#39;ve found sklearn LinearRegression&#39;s solution to be unstable under some cases.</span>
        <span class="c1"># The reason could be that sklearn calls lapack backend that depends on the build environment.</span>
        <span class="c1"># statsmodels uses simple QR decomposition/pseudo inverse to compute the solution,</span>
        <span class="c1"># and provides stabler solutions.</span>
        <span class="s2">&quot;linear&quot;</span><span class="p">:</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">,</span>
        <span class="s2">&quot;elastic_net&quot;</span><span class="p">:</span> <span class="n">ElasticNetCV</span><span class="p">,</span>
        <span class="s2">&quot;ridge&quot;</span><span class="p">:</span> <span class="n">RidgeCV</span><span class="p">,</span>
        <span class="s2">&quot;lasso&quot;</span><span class="p">:</span> <span class="n">LassoCV</span><span class="p">,</span>
        <span class="s2">&quot;sgd&quot;</span><span class="p">:</span> <span class="n">SGDRegressor</span><span class="p">,</span>  <span class="c1"># fits linear, elastic_net, ridge, lasso via SGD. Default is ridge with alpha = 0.0001</span>
        <span class="s2">&quot;lars&quot;</span><span class="p">:</span> <span class="n">LarsCV</span><span class="p">,</span>
        <span class="s2">&quot;lasso_lars&quot;</span><span class="p">:</span> <span class="n">LassoLarsCV</span><span class="p">,</span>
        <span class="s2">&quot;rf&quot;</span><span class="p">:</span> <span class="n">RandomForestRegressor</span><span class="p">,</span>
        <span class="s2">&quot;gradient_boosting&quot;</span><span class="p">:</span> <span class="n">GradientBoostingRegressor</span><span class="p">,</span>
        <span class="s2">&quot;quantile_regression&quot;</span><span class="p">:</span> <span class="n">QuantileRegression</span>
    <span class="p">}</span>

    <span class="c1"># for our purposes, we may want different defaults from those provided in the classes</span>
    <span class="c1"># sets the default `cv` and `n_estimators`</span>
    <span class="n">default_fit_algorithm_params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;statsmodels_ols&quot;</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(),</span>
        <span class="s2">&quot;statsmodels_wls&quot;</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(),</span>
        <span class="s2">&quot;statsmodels_gls&quot;</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(),</span>
        <span class="s2">&quot;statsmodels_glm&quot;</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Gamma</span><span class="p">()),</span>  <span class="c1"># default is gamma distribution</span>
        <span class="s2">&quot;linear&quot;</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(),</span>
        <span class="s2">&quot;elastic_net&quot;</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span>
        <span class="s2">&quot;ridge&quot;</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">alphas</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">30</span><span class="p">)),</span>  <span class="c1"># by default RidgeCV only has 3 candidate alphas, not enough</span>
        <span class="s2">&quot;lasso&quot;</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span>
        <span class="s2">&quot;sgd&quot;</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(),</span>
        <span class="s2">&quot;lars&quot;</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span>
        <span class="s2">&quot;lasso_lars&quot;</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span>
        <span class="s2">&quot;rf&quot;</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">),</span>
        <span class="s2">&quot;gradient_boosting&quot;</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(),</span>
        <span class="s2">&quot;quantile_regression&quot;</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">quantile</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># unregularized version modeling median</span>
    <span class="p">}</span>

    <span class="c1"># Re-standardizes the weights so that they sum up to data length</span>
    <span class="c1"># Note that in the case of no weights, each weight will be 1,</span>
    <span class="c1"># which also sums up to data length</span>
    <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">fit_algorithm</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;ridge&quot;</span><span class="p">,</span> <span class="s2">&quot;statsmodels_wls&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;sample weights are passed. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;However </span><span class="si">{</span><span class="n">fit_algorithm</span><span class="si">}</span><span class="s2"> does not support weighted regression.&quot;</span><span class="p">)</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">)</span> <span class="o">*</span> <span class="n">sample_weight</span> <span class="o">/</span> <span class="nb">sum</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">fit_algorithm</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">fit_algorithm_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The fit algorithm requested was not found: </span><span class="si">{</span><span class="n">fit_algorithm</span><span class="si">}</span><span class="s2">. &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;Must be one of </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">fit_algorithm_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># overwrites default params with those provided by user</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">default_fit_algorithm_params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">fit_algorithm</span><span class="p">,</span> <span class="p">{})</span>
    <span class="k">if</span> <span class="n">fit_algorithm_params</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">fit_algorithm_params</span><span class="p">)</span>

    <span class="c1"># ``ml_model`` refers to fitted machine-learning model object</span>
    <span class="k">if</span> <span class="s2">&quot;statsmodels&quot;</span> <span class="ow">in</span> <span class="n">fit_algorithm</span> <span class="ow">or</span> <span class="n">fit_algorithm</span> <span class="o">==</span> <span class="s2">&quot;linear&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">fit_algorithm</span> <span class="o">==</span> <span class="s2">&quot;statsmodels_wls&quot;</span> <span class="ow">and</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">ml_model</span> <span class="o">=</span> <span class="n">fit_algorithm_dict</span><span class="p">[</span><span class="n">fit_algorithm</span><span class="p">](</span>
                <span class="n">endog</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span>
                <span class="n">exog</span><span class="o">=</span><span class="n">x_train</span><span class="p">,</span>
                <span class="n">weights</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
                <span class="o">**</span><span class="n">params</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ml_model</span> <span class="o">=</span> <span class="n">fit_algorithm_dict</span><span class="p">[</span><span class="n">fit_algorithm</span><span class="p">](</span>
                <span class="n">endog</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span>
                <span class="n">exog</span><span class="o">=</span><span class="n">x_train</span><span class="p">,</span>
                <span class="o">**</span><span class="n">params</span><span class="p">)</span>
        <span class="n">ml_model</span> <span class="o">=</span> <span class="n">ml_model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
        <span class="c1"># Adds .coef_ and .intercept_ to statsmodels, so we could fetch parameters from .coef_ for all models.</span>
        <span class="c1"># Intercept is already included in params, setting .intercept_=0 in case it is needed.</span>
        <span class="n">ml_model</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">ml_model</span><span class="o">.</span><span class="n">params</span>
        <span class="n">ml_model</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ml_model</span> <span class="o">=</span> <span class="n">fit_algorithm_dict</span><span class="p">[</span><span class="n">fit_algorithm</span><span class="p">](</span><span class="o">**</span><span class="n">params</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">fit_algorithm</span> <span class="o">==</span> <span class="s2">&quot;ridge&quot;</span><span class="p">:</span>
            <span class="n">ml_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
                <span class="n">X</span><span class="o">=</span><span class="n">x_train</span><span class="p">,</span>
                <span class="n">y</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span>
                <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ml_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
                <span class="n">X</span><span class="o">=</span><span class="n">x_train</span><span class="p">,</span>
                <span class="n">y</span><span class="o">=</span><span class="n">y_train</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">ml_model</span>


<span class="k">def</span> <span class="nf">get_h_mat</span><span class="p">(</span><span class="n">x_mat</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Computes the H matrix given ``x_mat`` and ``alpha`` for linear and ridge regression.</span>
<span class="sd">    The formula is ``H = inv(X.T @ X + alpha * np.eye(p)) @ X.T``.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x_mat : `numpy.ndarray` or `pandas.DataFrame`</span>
<span class="sd">        Design matrix, dimension n by p.</span>
<span class="sd">    alpha : `float`</span>
<span class="sd">        The regularization term from the linear / ridge regression.</span>
<span class="sd">        Note that the OLS (ridge) estimator is ``inv(X.T @ X + alpha * np.eye(p)) @ X.T @ Y =: H @ Y``.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    h_mat : `numpy.ndarray`</span>
<span class="sd">        The H matrix as defined above. Dimension is p by n.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x_mat</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">XTX_alpha</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="n">alpha</span><span class="p">]</span> <span class="o">*</span> <span class="n">p</span><span class="p">)</span>
    <span class="n">log_cond</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span><span class="n">XTX_alpha</span><span class="p">))</span>
    <span class="n">digits_to_lose</span> <span class="o">=</span> <span class="mi">8</span>
    <span class="c1"># When `log_cond` is small, the matrix is full rank and not near singular,</span>
    <span class="c1"># in this case we should use `solve` for a positive definite matrix to optimize efficiency.</span>
    <span class="c1"># When `log_cond` is large, the matrix is near singular, we use `pinv` instead.</span>
    <span class="k">if</span> <span class="n">log_cond</span> <span class="o">&lt;</span> <span class="n">digits_to_lose</span><span class="p">:</span>
        <span class="n">h_mat</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">XTX_alpha</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">assume_a</span><span class="o">=</span><span class="s2">&quot;pos&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">h_mat</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinvh</span><span class="p">(</span><span class="n">XTX_alpha</span><span class="p">)</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span>
    <span class="k">return</span> <span class="n">h_mat</span>


<div class="viewcode-block" id="fit_ml_model"><a class="viewcode-back" href="../../../../pages/autodoc/doc.html#greykite.algo.common.ml_models.fit_ml_model">[docs]</a><span class="k">def</span> <span class="nf">fit_ml_model</span><span class="p">(</span>
        <span class="n">df</span><span class="p">,</span>
        <span class="n">model_formula_str</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">fit_algorithm</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span>
        <span class="n">fit_algorithm_params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">y_col</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">pred_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">min_admissible_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">max_admissible_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">uncertainty_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">normalize_method</span><span class="o">=</span><span class="s2">&quot;zero_to_one&quot;</span><span class="p">,</span>
        <span class="n">regression_weight_col</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">remove_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Fits predictive ML (machine learning) models to continuous</span>
<span class="sd">    response vector (given in ``y_col``)</span>
<span class="sd">    and returns fitted model.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    df : pd.DataFrame</span>
<span class="sd">        A data frame with the response vector (y) and the feature columns</span>
<span class="sd">        (``x_mat``).</span>
<span class="sd">    model_formula_str : str</span>
<span class="sd">        The prediction model formula string e.g. &quot;y~x1+x2+x3*x4&quot;.</span>
<span class="sd">        This is similar to R formulas.</span>
<span class="sd">        See https://patsy.readthedocs.io/en/latest/formulas.html#how-formulas-work.</span>
<span class="sd">    fit_algorithm : `str`, optional, default &quot;linear&quot;</span>
<span class="sd">        The type of predictive model used in fitting.</span>

<span class="sd">        See `~greykite.algo.common.ml_models.fit_model_via_design_matrix`</span>
<span class="sd">        for available options and their parameters.</span>
<span class="sd">    fit_algorithm_params : `dict` or None, optional, default None</span>
<span class="sd">        Parameters passed to the requested fit_algorithm.</span>
<span class="sd">        If None, uses the defaults in `~greykite.algo.common.ml_models.fit_model_via_design_matrix`.</span>
<span class="sd">    y_col : str</span>
<span class="sd">        The column name which has the value of interest to be forecasted</span>
<span class="sd">        If the model_formula_str is not passed, ``y_col`` e.g. [&quot;y&quot;]</span>
<span class="sd">        is used as the response vector column</span>
<span class="sd">    pred_cols : List[str]</span>
<span class="sd">        The names of the feature columns</span>
<span class="sd">        If the ``model_formula_str`` is not passed, ``pred_cols`` e.g.</span>
<span class="sd">        [&quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;] is used as the design matrix columns</span>
<span class="sd">    min_admissible_value : Optional[Union[int, float, double]]</span>
<span class="sd">        the minimum admissible value for the ``predict`` function to return</span>
<span class="sd">    max_admissible_value : Optional[Union[int, float, double]]</span>
<span class="sd">        the maximum admissible value for the ``predict`` function to return</span>
<span class="sd">    uncertainty_dict : `dict` or None</span>
<span class="sd">        If passed as a dictionary an uncertainty model will be fit.</span>
<span class="sd">        The items in the dictionary are:</span>

<span class="sd">            ``&quot;uncertainty_method&quot;`` : `str`</span>
<span class="sd">                the title of the method</span>
<span class="sd">                as of now only &quot;simple_conditional_residuals&quot; is implemented</span>
<span class="sd">                which calculates CIs by using residuals</span>
<span class="sd">            ``&quot;params&quot;`` : `dict`</span>
<span class="sd">                A dictionary of parameters needed for the ``uncertainty_method``</span>
<span class="sd">                requested</span>

<span class="sd">    normalize_method : `str` or None, default &quot;zero_to_one&quot;</span>
<span class="sd">        If a string is provided, it will be used as the normalization method</span>
<span class="sd">        in `~greykite.common.features.normalize.normalize_df`, passed via</span>
<span class="sd">        the argument ``method``.</span>
<span class="sd">        Available options are: &quot;zero_to_one&quot;, &quot;statistical&quot;, &quot;minus_half_to_half&quot;, &quot;zero_at_origin&quot;.</span>
<span class="sd">        If None, no normalization will be performed.</span>
<span class="sd">        See that function for more details.</span>
<span class="sd">    regression_weight_col : `str` or None, default None</span>
<span class="sd">        The column name for the weights to be used in weighted regression version</span>
<span class="sd">        of applicable machine-learning models.</span>
<span class="sd">    remove_intercept : `bool`, default False</span>
<span class="sd">        Whether to remove explicit and implicit intercepts.</span>
<span class="sd">        By default, `patsy` will make the design matrix always full rank.</span>
<span class="sd">        It will always include an intercept term unless we specify &quot;-1&quot; or &quot;+0&quot;.</span>
<span class="sd">        However, if there are categorical variables, even we specify &quot;-1&quot; or &quot;+0&quot;,</span>
<span class="sd">        it will include an implicit intercept by adding all levels of a categorical</span>
<span class="sd">        variable into the design matrix.</span>
<span class="sd">        Sometimes we don&#39;t want this to happen.</span>
<span class="sd">        Setting this parameter to True will remove both explicit and implicit intercepts.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    trained_model : `dict`</span>
<span class="sd">        Trained model dictionary with keys:</span>

<span class="sd">            - &quot;y&quot; : response values</span>
<span class="sd">            - &quot;x_design_info&quot; : design matrix information</span>
<span class="sd">            - &quot;ml_model&quot; : A trained model with predict method</span>
<span class="sd">            - &quot;uncertainty_model&quot; : `dict`</span>
<span class="sd">                The returned uncertainty_model dict from</span>
<span class="sd">                `~greykite.algo.uncertainty.conditional.conf_interval.conf_interval`.</span>
<span class="sd">            - &quot;ml_model_summary&quot;: model summary</span>
<span class="sd">            - &quot;y_col&quot; : response columns</span>
<span class="sd">            - &quot;x_mat &quot;: design matrix</span>
<span class="sd">            - &quot;min_admissible_value&quot; : minimum acceptable value</span>
<span class="sd">            - &quot;max_admissible_value&quot; : maximum acceptable value</span>
<span class="sd">            - &quot;normalize_df_func&quot; : normalization function</span>
<span class="sd">            - &quot;regression_weight_col&quot; : regression weight column</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Builds model matrices.</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">design_mat_from_formula</span><span class="p">(</span>
        <span class="n">df</span><span class="o">=</span><span class="n">df</span><span class="p">,</span>
        <span class="n">model_formula_str</span><span class="o">=</span><span class="n">model_formula_str</span><span class="p">,</span>
        <span class="n">y_col</span><span class="o">=</span><span class="n">y_col</span><span class="p">,</span>
        <span class="n">pred_cols</span><span class="o">=</span><span class="n">pred_cols</span><span class="p">,</span>
        <span class="n">remove_intercept</span><span class="o">=</span><span class="n">remove_intercept</span>
    <span class="p">)</span>

    <span class="n">y</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">]</span>
    <span class="n">y_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">y_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">x_mat</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="s2">&quot;x_mat&quot;</span><span class="p">]</span>
    <span class="n">y_col</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="s2">&quot;y_col&quot;</span><span class="p">]</span>
    <span class="n">x_design_info</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="s2">&quot;x_design_info&quot;</span><span class="p">]</span>
    <span class="n">drop_intercept_col</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="s2">&quot;drop_intercept_col&quot;</span><span class="p">]</span>

    <span class="n">normalize_df_func</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">normalize_method</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="s2">&quot;Intercept&quot;</span> <span class="ow">in</span> <span class="p">(</span><span class="n">x_mat</span><span class="o">.</span><span class="n">columns</span><span class="p">):</span>
            <span class="n">cols</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">x_mat</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span> <span class="k">if</span> <span class="n">col</span> <span class="o">!=</span> <span class="s2">&quot;Intercept&quot;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">cols</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">x_mat</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
        <span class="n">normalize_info</span> <span class="o">=</span> <span class="n">normalize_df</span><span class="p">(</span>
            <span class="n">df</span><span class="o">=</span><span class="n">x_mat</span><span class="p">[</span><span class="n">cols</span><span class="p">],</span>
            <span class="n">method</span><span class="o">=</span><span class="n">normalize_method</span><span class="p">,</span>
            <span class="n">drop_degenerate_cols</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">replace_zero_denom</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">x_mat</span><span class="p">[</span><span class="n">cols</span><span class="p">]</span> <span class="o">=</span> <span class="n">normalize_info</span><span class="p">[</span><span class="s2">&quot;normalized_df&quot;</span><span class="p">]</span>
        <span class="n">x_mat</span> <span class="o">=</span> <span class="n">x_mat</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">normalize_df_func</span> <span class="o">=</span> <span class="n">normalize_info</span><span class="p">[</span><span class="s2">&quot;normalize_df_func&quot;</span><span class="p">]</span>

    <span class="n">sample_weight</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">regression_weight_col</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">df</span><span class="p">[</span><span class="n">regression_weight_col</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Weights can not be negative. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;The column </span><span class="si">{</span><span class="n">regression_weight_col</span><span class="si">}</span><span class="s2"> includes negative values.&quot;</span><span class="p">)</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">regression_weight_col</span><span class="p">]</span>

    <span class="c1"># Prediction model generated by using all observed data.</span>
    <span class="n">ml_model</span> <span class="o">=</span> <span class="n">fit_model_via_design_matrix</span><span class="p">(</span>
        <span class="n">x_train</span><span class="o">=</span><span class="n">x_mat</span><span class="p">,</span>
        <span class="n">y_train</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
        <span class="n">fit_algorithm</span><span class="o">=</span><span class="n">fit_algorithm</span><span class="p">,</span>
        <span class="n">fit_algorithm_params</span><span class="o">=</span><span class="n">fit_algorithm_params</span><span class="p">,</span>
        <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>

    <span class="c1"># Obtains `alpha`, `p_effective`, `h_mat` (H), and `sigma_scaler`.</span>
    <span class="c1"># See comments below the variables.</span>
    <span class="c1"># Read more at https://online.stat.psu.edu/stat508/lesson/5/5.1 or</span>
    <span class="c1"># book: Applied Regression Analysis by Norman R. Draper, Harry Smith.</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The regularization term from the linear / ridge regression.</span>
<span class="sd">    Note that the OLS (ridge) estimator is ``inv(X.T @ X + alpha * np.eye(p)) @ X.T @ Y =: H @ Y``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p_effective</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Effective number of parameters.</span>
<span class="sd">    In linear regressions, it is also equal to ``trace(X @ H)``, where H is defined above.</span>
<span class="sd">    ``X @ H`` is also called the hat matrix.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">h_mat</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The H matrix (p by n) in linear regression estimator, as defined above.</span>
<span class="sd">    Note that H is not necessarily of full-rank p even in ridge regression.</span>
<span class="sd">    ``H = inv(X.T @ X + alpha * np.eye(p)) @ X.T``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sigma_scaler</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Theoretical scaler of the estimated sigma.</span>
<span class="sd">    Volatility model estimates sigma by taking the sample standard deviation, and</span>
<span class="sd">    we need to scale it by ``np.sqrt((n_train - 1) / (n_train - p_effective))`` to obtain</span>
<span class="sd">    an unbiased estimator.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x_mean</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Column mean of ``x_mat`` as a row vector.</span>
<span class="sd">    This is stored and used in ridge regression to compute the prediction intervals.</span>
<span class="sd">    In other methods, it is set to `None`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">fit_algorithm</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;ridge&quot;</span><span class="p">,</span> <span class="s2">&quot;linear&quot;</span><span class="p">]:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x_mat</span><span class="p">)</span>
        <span class="n">n_train</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
        <span class="c1"># Extracts `alpha` from the fitted ML model.</span>
        <span class="c1"># In linear regression, the rank of the design matrix is `p_effective`,</span>
        <span class="c1"># but `RidgeCV` we need to manually derive it by taking the trace.</span>
        <span class="c1"># Note that `RidgeCV` centers `X` and `Y` before fitting, hence we need to center `X` too.</span>
        <span class="k">if</span> <span class="n">fit_algorithm</span> <span class="o">==</span> <span class="s2">&quot;ridge&quot;</span><span class="p">:</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="n">ml_model</span><span class="o">.</span><span class="n">alpha_</span>
            <span class="n">x_mean</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">x_mean</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">p_effective</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">matrix_rank</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="c1"># Computes `h_mat` (H, p x n).</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">h_mat</span> <span class="o">=</span> <span class="n">get_h_mat</span><span class="p">(</span><span class="n">x_mat</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">fit_algorithm</span> <span class="o">==</span> <span class="s2">&quot;ridge&quot;</span><span class="p">:</span>
                <span class="c1"># Computes the effective number of parameters.</span>
                <span class="c1"># Note that `p_effective` is the trace of `X @ h_mat` plus 1 for intercept, however</span>
                <span class="c1"># computing `trace(h_mat @ X)` is more efficient due to much faster matrix multiplication.</span>
                <span class="n">p_effective</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">h_mat</span> <span class="o">@</span> <span class="n">X</span><span class="p">),</span> <span class="mi">6</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># Avoids floating issues e.g. 1.9999999999999998.</span>
        <span class="k">except</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">LinAlgError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">message</span> <span class="o">=</span> <span class="n">traceback</span><span class="o">.</span><span class="n">format_exc</span><span class="p">()</span>
            <span class="n">warning_msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Error &#39;</span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&#39; occurred when computing `h_mat`, no variance scaling is done!</span><span class="se">\n</span><span class="s2">&quot;</span> \
                          <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">message</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="n">log_message</span><span class="p">(</span><span class="n">warning_msg</span><span class="p">,</span> <span class="n">LoggingLevelEnum</span><span class="o">.</span><span class="n">WARNING</span><span class="p">)</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">warning_msg</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">p_effective</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">round</span><span class="p">(</span><span class="n">p_effective</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">n_train</span><span class="p">:</span>
            <span class="c1"># Computes scaler on sigma estimate.</span>
            <span class="n">sigma_scaler</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">n_train</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_train</span> <span class="o">-</span> <span class="n">p_effective</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Zero degrees of freedom (</span><span class="si">{</span><span class="n">n_train</span><span class="si">}</span><span class="s2">-</span><span class="si">{</span><span class="n">p_effective</span><span class="si">}</span><span class="s2">) or the inverse solver failed. &quot;</span>
                          <span class="sa">f</span><span class="s2">&quot;Likely caused by singular `X.T @ X + alpha * np.eye(p)`. &quot;</span>
                          <span class="sa">f</span><span class="s2">&quot;Please check </span><span class="se">\&quot;</span><span class="s2">x_mat</span><span class="se">\&quot;</span><span class="s2">, </span><span class="se">\&quot;</span><span class="s2">alpha</span><span class="se">\&quot;</span><span class="s2">. &quot;</span>
                          <span class="sa">f</span><span class="s2">&quot;`sigma_scaler` cannot be computed!&quot;</span><span class="p">)</span>

    <span class="c1"># Uncertainty model is fitted if `uncertainty_dict` is passed.</span>
    <span class="n">uncertainty_model</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">uncertainty_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">uncertainty_method</span> <span class="o">=</span> <span class="n">uncertainty_dict</span><span class="p">[</span><span class="s2">&quot;uncertainty_method&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">uncertainty_method</span> <span class="o">==</span> <span class="s2">&quot;simple_conditional_residuals&quot;</span><span class="p">:</span>
            <span class="c1"># Resets index to match behavior of predict before assignment.</span>
            <span class="n">new_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="p">(</span><span class="n">new_x_mat</span><span class="p">,)</span> <span class="o">=</span> <span class="n">patsy</span><span class="o">.</span><span class="n">build_design_matrices</span><span class="p">(</span>
                <span class="p">[</span><span class="n">x_design_info</span><span class="p">],</span>
                <span class="n">data</span><span class="o">=</span><span class="n">new_df</span><span class="p">,</span>
                <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;dataframe&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">drop_intercept_col</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">new_x_mat</span> <span class="o">=</span> <span class="n">new_x_mat</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="n">drop_intercept_col</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">normalize_df_func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="s2">&quot;Intercept&quot;</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">x_mat</span><span class="o">.</span><span class="n">columns</span><span class="p">):</span>
                    <span class="n">cols</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">x_mat</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span> <span class="k">if</span> <span class="n">col</span> <span class="o">!=</span> <span class="s2">&quot;Intercept&quot;</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">cols</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">x_mat</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
                <span class="n">new_x_mat</span><span class="p">[</span><span class="n">cols</span><span class="p">]</span> <span class="o">=</span> <span class="n">normalize_df_func</span><span class="p">(</span><span class="n">new_x_mat</span><span class="p">[</span><span class="n">cols</span><span class="p">])</span>
            <span class="n">new_x_mat</span> <span class="o">=</span> <span class="n">new_x_mat</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">new_df</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">y_col</span><span class="si">}</span><span class="s2">_pred&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ml_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">new_x_mat</span><span class="p">)</span>
            <span class="n">new_df</span><span class="p">[</span><span class="n">RESIDUAL_COL</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_df</span><span class="p">[</span><span class="n">y_col</span><span class="p">]</span> <span class="o">-</span> <span class="n">new_df</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">y_col</span><span class="si">}</span><span class="s2">_pred&quot;</span><span class="p">]</span>

            <span class="c1"># Re-assigns some param defaults for function `conf_interval`</span>
            <span class="c1"># with values best suited to this case.</span>
            <span class="n">conf_interval_params</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;quantiles&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.975</span><span class="p">],</span>
                <span class="s2">&quot;sample_size_thresh&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">}</span>

            <span class="k">if</span> <span class="n">uncertainty_dict</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">conf_interval_params</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">uncertainty_dict</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">])</span>
            <span class="n">uncertainty_model</span> <span class="o">=</span> <span class="n">conf_interval</span><span class="p">(</span>
                <span class="n">df</span><span class="o">=</span><span class="n">new_df</span><span class="p">,</span>
                <span class="n">distribution_col</span><span class="o">=</span><span class="n">RESIDUAL_COL</span><span class="p">,</span>
                <span class="n">offset_col</span><span class="o">=</span><span class="n">y_col</span><span class="p">,</span>
                <span class="n">sigma_scaler</span><span class="o">=</span><span class="n">sigma_scaler</span><span class="p">,</span>
                <span class="n">h_mat</span><span class="o">=</span><span class="n">h_mat</span><span class="p">,</span>
                <span class="n">x_mean</span><span class="o">=</span><span class="n">x_mean</span><span class="p">,</span>
                <span class="n">min_admissible_value</span><span class="o">=</span><span class="n">min_admissible_value</span><span class="p">,</span>
                <span class="n">max_admissible_value</span><span class="o">=</span><span class="n">max_admissible_value</span><span class="p">,</span>
                <span class="o">**</span><span class="n">conf_interval_params</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;uncertainty method: </span><span class="si">{</span><span class="n">uncertainty_method</span><span class="si">}</span><span class="s2"> is not implemented&quot;</span><span class="p">)</span>

    <span class="c1"># We get the model summary for a subset of models</span>
    <span class="c1"># where summary is available (`statsmodels` module),</span>
    <span class="c1"># or summary can be constructed (a subset of models from `sklearn`).</span>
    <span class="n">ml_model_summary</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="s2">&quot;statsmodels&quot;</span> <span class="ow">in</span> <span class="n">fit_algorithm</span><span class="p">:</span>
        <span class="n">ml_model_summary</span> <span class="o">=</span> <span class="n">ml_model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">ml_model</span><span class="p">,</span> <span class="s2">&quot;coef_&quot;</span><span class="p">):</span>
        <span class="n">var_names</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">x_mat</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
        <span class="n">coefs</span> <span class="o">=</span> <span class="n">ml_model</span><span class="o">.</span><span class="n">coef_</span>
        <span class="n">ml_model_summary</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
            <span class="s2">&quot;variable&quot;</span><span class="p">:</span> <span class="n">var_names</span><span class="p">,</span>
            <span class="s2">&quot;coef&quot;</span><span class="p">:</span> <span class="n">coefs</span><span class="p">})</span>

    <span class="n">trained_model</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="n">y</span><span class="p">,</span>
        <span class="s2">&quot;y_mean&quot;</span><span class="p">:</span> <span class="n">y_mean</span><span class="p">,</span>
        <span class="s2">&quot;y_std&quot;</span><span class="p">:</span> <span class="n">y_std</span><span class="p">,</span>
        <span class="s2">&quot;x_design_info&quot;</span><span class="p">:</span> <span class="n">x_design_info</span><span class="p">,</span>
        <span class="s2">&quot;ml_model&quot;</span><span class="p">:</span> <span class="n">ml_model</span><span class="p">,</span>
        <span class="s2">&quot;uncertainty_model&quot;</span><span class="p">:</span> <span class="n">uncertainty_model</span><span class="p">,</span>
        <span class="s2">&quot;ml_model_summary&quot;</span><span class="p">:</span> <span class="n">ml_model_summary</span><span class="p">,</span>
        <span class="s2">&quot;y_col&quot;</span><span class="p">:</span> <span class="n">y_col</span><span class="p">,</span>
        <span class="s2">&quot;x_mat&quot;</span><span class="p">:</span> <span class="n">x_mat</span><span class="p">,</span>
        <span class="s2">&quot;min_admissible_value&quot;</span><span class="p">:</span> <span class="n">min_admissible_value</span><span class="p">,</span>
        <span class="s2">&quot;max_admissible_value&quot;</span><span class="p">:</span> <span class="n">max_admissible_value</span><span class="p">,</span>
        <span class="s2">&quot;normalize_df_func&quot;</span><span class="p">:</span> <span class="n">normalize_df_func</span><span class="p">,</span>
        <span class="s2">&quot;regression_weight_col&quot;</span><span class="p">:</span> <span class="n">regression_weight_col</span><span class="p">,</span>
        <span class="s2">&quot;drop_intercept_col&quot;</span><span class="p">:</span> <span class="n">drop_intercept_col</span><span class="p">,</span>
        <span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="n">alpha</span><span class="p">,</span>
        <span class="s2">&quot;h_mat&quot;</span><span class="p">:</span> <span class="n">h_mat</span><span class="p">,</span>
        <span class="s2">&quot;p_effective&quot;</span><span class="p">:</span> <span class="n">p_effective</span><span class="p">,</span>
        <span class="s2">&quot;sigma_scaler&quot;</span><span class="p">:</span> <span class="n">sigma_scaler</span><span class="p">}</span>

    <span class="k">if</span> <span class="n">uncertainty_dict</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fitted_df</span> <span class="o">=</span> <span class="n">predict_ml</span><span class="p">(</span>
            <span class="n">fut_df</span><span class="o">=</span><span class="n">df</span><span class="p">,</span>
            <span class="n">trained_model</span><span class="o">=</span><span class="n">trained_model</span><span class="p">)[</span><span class="s2">&quot;fut_df&quot;</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">fitted_df</span> <span class="o">=</span> <span class="n">predict_ml_with_uncertainty</span><span class="p">(</span>
            <span class="n">fut_df</span><span class="o">=</span><span class="n">df</span><span class="p">,</span>
            <span class="n">trained_model</span><span class="o">=</span><span class="n">trained_model</span><span class="p">)[</span><span class="s2">&quot;fut_df&quot;</span><span class="p">]</span>

    <span class="n">trained_model</span><span class="p">[</span><span class="s2">&quot;fitted_df&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fitted_df</span>

    <span class="k">return</span> <span class="n">trained_model</span></div>


<span class="k">def</span> <span class="nf">predict_ml</span><span class="p">(</span>
        <span class="n">fut_df</span><span class="p">,</span>
        <span class="n">trained_model</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns predictions on new data using the machine-learning (ml) model</span>
<span class="sd">    fitted via ``fit_ml_model``.</span>

<span class="sd">    :param fut_df: `pd.DataFrame`</span>
<span class="sd">        Input data for prediction.</span>
<span class="sd">        Must have all columns used for training,</span>
<span class="sd">        specified in ``model_formula_str`` or ``pred_cols``</span>
<span class="sd">    :param trained_model: `dict`</span>
<span class="sd">        A trained model returned from ``fit_ml_model``</span>
<span class="sd">    :return: `dict`</span>
<span class="sd">        A dictionary with following keys</span>

<span class="sd">        - &quot;fut_df&quot;: `pd.DataFrame`</span>
<span class="sd">            Input data with ``y_col`` set to the predicted values</span>
<span class="sd">        - &quot;x_mat&quot;: `patsy.design_info.DesignMatrix`</span>
<span class="sd">            Design matrix of the predictive model</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">y_col</span> <span class="o">=</span> <span class="n">trained_model</span><span class="p">[</span><span class="s2">&quot;y_col&quot;</span><span class="p">]</span>
    <span class="n">ml_model</span> <span class="o">=</span> <span class="n">trained_model</span><span class="p">[</span><span class="s2">&quot;ml_model&quot;</span><span class="p">]</span>
    <span class="n">x_design_info</span> <span class="o">=</span> <span class="n">trained_model</span><span class="p">[</span><span class="s2">&quot;x_design_info&quot;</span><span class="p">]</span>
    <span class="n">drop_intercept_col</span> <span class="o">=</span> <span class="n">trained_model</span><span class="p">[</span><span class="s2">&quot;drop_intercept_col&quot;</span><span class="p">]</span>
    <span class="n">min_admissible_value</span> <span class="o">=</span> <span class="n">trained_model</span><span class="p">[</span><span class="s2">&quot;min_admissible_value&quot;</span><span class="p">]</span>
    <span class="n">max_admissible_value</span> <span class="o">=</span> <span class="n">trained_model</span><span class="p">[</span><span class="s2">&quot;max_admissible_value&quot;</span><span class="p">]</span>

    <span class="c1"># reset indices to avoid issues when adding new cols</span>
    <span class="n">fut_df</span> <span class="o">=</span> <span class="n">fut_df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">(</span><span class="n">x_mat</span><span class="p">,)</span> <span class="o">=</span> <span class="n">patsy</span><span class="o">.</span><span class="n">build_design_matrices</span><span class="p">(</span>
        <span class="p">[</span><span class="n">x_design_info</span><span class="p">],</span>
        <span class="n">data</span><span class="o">=</span><span class="n">fut_df</span><span class="p">,</span>
        <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;dataframe&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">drop_intercept_col</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x_mat</span> <span class="o">=</span> <span class="n">x_mat</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="n">drop_intercept_col</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">trained_model</span><span class="p">[</span><span class="s2">&quot;normalize_df_func&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="s2">&quot;Intercept&quot;</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">x_mat</span><span class="o">.</span><span class="n">columns</span><span class="p">):</span>
            <span class="n">cols</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">x_mat</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span> <span class="k">if</span> <span class="n">col</span> <span class="o">!=</span> <span class="s2">&quot;Intercept&quot;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">cols</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">x_mat</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
        <span class="n">x_mat</span><span class="p">[</span><span class="n">cols</span><span class="p">]</span> <span class="o">=</span> <span class="n">trained_model</span><span class="p">[</span><span class="s2">&quot;normalize_df_func&quot;</span><span class="p">](</span><span class="n">x_mat</span><span class="p">[</span><span class="n">cols</span><span class="p">])</span>
    <span class="n">x_mat</span> <span class="o">=</span> <span class="n">x_mat</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">ml_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_mat</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">min_admissible_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">max_admissible_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span>
            <span class="n">a</span><span class="o">=</span><span class="n">y_pred</span><span class="p">,</span>
            <span class="n">a_min</span><span class="o">=</span><span class="n">min_admissible_value</span><span class="p">,</span>
            <span class="n">a_max</span><span class="o">=</span><span class="n">max_admissible_value</span><span class="p">)</span>
    <span class="n">fut_df</span><span class="p">[</span><span class="n">y_col</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_pred</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;fut_df&quot;</span><span class="p">:</span> <span class="n">fut_df</span><span class="p">,</span>
        <span class="s2">&quot;x_mat&quot;</span><span class="p">:</span> <span class="n">x_mat</span><span class="p">}</span>


<span class="k">def</span> <span class="nf">predict_ml_with_uncertainty</span><span class="p">(</span>
        <span class="n">fut_df</span><span class="p">,</span>
        <span class="n">trained_model</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns predictions and prediction intervals on new data using</span>
<span class="sd">    the machine-learning (ml) model</span>
<span class="sd">    fitted via ``fit_ml_model`` and the uncertainty model fitted via</span>
<span class="sd">    ``greykite.algo.uncertainty.conditional.conf_interval.conf_interval``</span>

<span class="sd">    :param fut_df: `pd.DataFrame`</span>
<span class="sd">        Input data for prediction.</span>
<span class="sd">        Must have all columns specified by</span>
<span class="sd">        ``model_formula_str`` or ``pred_cols``</span>
<span class="sd">    :param trained_model: `dict`</span>
<span class="sd">        A trained model returned from ``fit_ml_model``</span>
<span class="sd">    :return: `dict`</span>
<span class="sd">        A dictionary with following keys</span>

<span class="sd">        - &quot;fut_df&quot;: `pd.DataFrame`</span>
<span class="sd">            Input data with ``y_col`` set to the predicted values</span>
<span class="sd">        - &quot;x_mat&quot;: `patsy.design_info.DesignMatrix`</span>
<span class="sd">            Design matrix of the predictive model</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Gets point predictions.</span>
    <span class="n">fut_df</span> <span class="o">=</span> <span class="n">fut_df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">y_col</span> <span class="o">=</span> <span class="n">trained_model</span><span class="p">[</span><span class="s2">&quot;y_col&quot;</span><span class="p">]</span>
    <span class="n">pred_res</span> <span class="o">=</span> <span class="n">predict_ml</span><span class="p">(</span>
        <span class="n">fut_df</span><span class="o">=</span><span class="n">fut_df</span><span class="p">,</span>
        <span class="n">trained_model</span><span class="o">=</span><span class="n">trained_model</span><span class="p">)</span>

    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pred_res</span><span class="p">[</span><span class="s2">&quot;fut_df&quot;</span><span class="p">][</span><span class="n">y_col</span><span class="p">]</span>
    <span class="n">x_mat</span> <span class="o">=</span> <span class="n">pred_res</span><span class="p">[</span><span class="s2">&quot;x_mat&quot;</span><span class="p">]</span>

    <span class="n">fut_df</span><span class="p">[</span><span class="n">y_col</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_pred</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="c1"># Applies uncertainty model.</span>
    <span class="n">pred_df_with_uncertainty</span> <span class="o">=</span> <span class="n">predict_ci</span><span class="p">(</span>
        <span class="n">fut_df</span><span class="p">,</span>
        <span class="n">trained_model</span><span class="p">[</span><span class="s2">&quot;uncertainty_model&quot;</span><span class="p">],</span>
        <span class="n">x_mat</span><span class="o">=</span><span class="n">x_mat</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;fut_df&quot;</span><span class="p">:</span> <span class="n">pred_df_with_uncertainty</span><span class="p">,</span>
        <span class="s2">&quot;x_mat&quot;</span><span class="p">:</span> <span class="n">x_mat</span><span class="p">}</span>


<div class="viewcode-block" id="fit_ml_model_with_evaluation"><a class="viewcode-back" href="../../../../pages/autodoc/doc.html#greykite.algo.common.ml_models.fit_ml_model_with_evaluation">[docs]</a><span class="k">def</span> <span class="nf">fit_ml_model_with_evaluation</span><span class="p">(</span>
        <span class="n">df</span><span class="p">,</span>
        <span class="n">model_formula_str</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">y_col</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">pred_cols</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">fit_algorithm</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span>
        <span class="n">fit_algorithm_params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">ind_train</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">ind_test</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">training_fraction</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
        <span class="n">randomize_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">min_admissible_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">max_admissible_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">uncertainty_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">normalize_method</span><span class="o">=</span><span class="s2">&quot;zero_to_one&quot;</span><span class="p">,</span>
        <span class="n">regression_weight_col</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">remove_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Fits prediction models to continuous response vector (y)</span>
<span class="sd">    and report results.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    df : `pandas.DataFrame`</span>
<span class="sd">        A data frame with the response vector (y) and the feature columns (``x_mat``)</span>
<span class="sd">    model_formula_str : `str`</span>
<span class="sd">        The prediction model formula e.g. &quot;y~x1+x2+x3*x4&quot;.</span>
<span class="sd">        This is similar to R language (https://www.r-project.org/) formulas.</span>
<span class="sd">        See https://patsy.readthedocs.io/en/latest/formulas.html#how-formulas-work.</span>
<span class="sd">    y_col : `str`</span>
<span class="sd">        The column name which has the value of interest to be forecasted</span>
<span class="sd">        If the ``model_formula_str`` is not passed, ``y_col`` e.g. [&quot;y&quot;] is used as the response</span>
<span class="sd">        vector column</span>
<span class="sd">    pred_cols : `list` [`str`]</span>
<span class="sd">        The names of the feature columns</span>
<span class="sd">        If the ``model_formula_str`` is not passed, ``pred_cols`` e.g. [&quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;]</span>
<span class="sd">        is used as the design matrix columns</span>
<span class="sd">    fit_algorithm : `str`, optional, default &quot;linear&quot;</span>
<span class="sd">        The type of predictive model used in fitting.</span>

<span class="sd">        See `~greykite.algo.common.ml_models.fit_model_via_design_matrix`</span>
<span class="sd">        for available options and their parameters.</span>
<span class="sd">    fit_algorithm_params : `dict` or None, optional, default None</span>
<span class="sd">        Parameters passed to the requested fit_algorithm.</span>
<span class="sd">        If None, uses the defaults in `~greykite.algo.common.ml_models.fit_model_via_design_matrix`.</span>
<span class="sd">    ind_train : `list` [`int`]</span>
<span class="sd">        The index (row number) of the training set</span>
<span class="sd">    ind_test : `list` [`int`]</span>
<span class="sd">        The index (row number) of the test set</span>
<span class="sd">    training_fraction : `float`, between 0.0 and 1.0</span>
<span class="sd">        The fraction of data used for training</span>
<span class="sd">        This is invoked if ind_train and ind_test are not passed</span>
<span class="sd">        If this is also None or 1.0, then we skip testing</span>
<span class="sd">        and train on the entire dataset</span>
<span class="sd">    randomize_training : `bool`</span>
<span class="sd">        If True, then the training and the test sets will be randomized</span>
<span class="sd">        rather than in chronological order</span>
<span class="sd">    min_admissible_value : Optional[Union[int, float, double]]</span>
<span class="sd">        The minimum admissible value for the ``predict`` function to return</span>
<span class="sd">    max_admissible_value : Optional[Union[int, float, double]]</span>
<span class="sd">        The maximum admissible value for the ``predict`` function to return</span>
<span class="sd">    uncertainty_dict: `dict` or None</span>
<span class="sd">        If passed as a dictionary an uncertainty model will be fit.</span>
<span class="sd">        The items in the dictionary are:</span>

<span class="sd">            ``&quot;uncertainty_method&quot;`` : `str`</span>
<span class="sd">                the title of the method</span>
<span class="sd">                as of now only &quot;simple_conditional_residuals&quot; is implemented</span>
<span class="sd">                which calculates CIs by using residuals</span>
<span class="sd">            ``&quot;params&quot;`` : `dict`</span>
<span class="sd">                A dictionary of parameters needed for the ``uncertainty_method``</span>
<span class="sd">                requested</span>

<span class="sd">    normalize_method : `str` or None, default &quot;zero_to_one&quot;</span>
<span class="sd">        If a string is provided, it will be used as the normalization method</span>
<span class="sd">        in `~greykite.common.features.normalize.normalize_df`, passed via</span>
<span class="sd">        the argument ``method``.</span>
<span class="sd">        Available options are: &quot;zero_to_one&quot;, &quot;statistical&quot;, &quot;minus_half_to_half&quot;, &quot;zero_at_origin&quot;.</span>
<span class="sd">        If None, no normalization will be performed.</span>
<span class="sd">        See that function for more details.</span>
<span class="sd">    regression_weight_col : `str` or None, default None</span>
<span class="sd">        The column name for the weights to be used in weighted regression version</span>
<span class="sd">        of applicable machine-learning models.</span>
<span class="sd">    remove_intercept : `bool`, default False</span>
<span class="sd">        Whether to remove explicit and implicit intercepts.</span>
<span class="sd">        By default, `patsy` will make the design matrix always full rank.</span>
<span class="sd">        It will always include an intercept term unless we specify &quot;-1&quot; or &quot;+0&quot;.</span>
<span class="sd">        However, if there are categorical variables, even we specify &quot;-1&quot; or &quot;+0&quot;,</span>
<span class="sd">        it will include an implicit intercept by adding all levels of a categorical</span>
<span class="sd">        variable into the design matrix.</span>
<span class="sd">        Sometimes we don&#39;t want this to happen.</span>
<span class="sd">        Setting this parameter to True will remove both explicit and implicit intercepts.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    trained_model : `dict`</span>
<span class="sd">        Trained model dictionary with the following keys.</span>

<span class="sd">            &quot;ml_model&quot;: A trained model object</span>
<span class="sd">            &quot;summary&quot;: Summary of the final model trained on all data</span>
<span class="sd">            &quot;x_mat&quot;: Feature vectors matrix used for training of full data (rows of ``df`` with NA are dropped)</span>
<span class="sd">            &quot;y&quot;: Response vector for training and testing (rows of ``df`` with NA are dropped).</span>
<span class="sd">                The index corresponds to selected rows in the input ``df``.</span>
<span class="sd">            &quot;y_train&quot;: Response vector used for training</span>
<span class="sd">            &quot;y_train_pred&quot;: Predicted values of ``y_train``</span>
<span class="sd">            &quot;training_evaluation&quot;: score function value of ``y_train`` and ``y_train_pred``</span>
<span class="sd">            &quot;y_test&quot;: Response vector used for testing</span>
<span class="sd">            &quot;y_test_pred&quot;: Predicted values of ``y_test``</span>
<span class="sd">            &quot;test_evaluation&quot;: score function value of ``y_test`` and ``y_test_pred``</span>
<span class="sd">            &quot;uncertainty_model&quot;: `dict`</span>
<span class="sd">                The returned uncertainty_model dict from</span>
<span class="sd">                `~greykite.algo.uncertainty.conditional.conf_interval.conf_interval`.</span>
<span class="sd">            &quot;plt_compare_test&quot;: plot function to compare ``y_test`` and ``y_test_pred``,</span>
<span class="sd">            &quot;plt_pred&quot;: plot function to compare</span>
<span class="sd">                ``y_train``, ``y_train_pred``, ``y_test`` and ``y_test_pred``.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># to avoid pandas unnecessary warnings due to chain assignment</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">mode</span><span class="o">.</span><span class="n">chained_assignment</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># dropping NAs</span>
    <span class="k">if</span> <span class="n">df</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
        <span class="n">nrows_original</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>  <span class="c1"># preserves index</span>
        <span class="n">nrows</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The data frame included </span><span class="si">{</span><span class="n">nrows_original</span><span class="o">-</span><span class="n">nrows</span><span class="si">}</span><span class="s2"> row(s) with NAs which were removed for model fitting.&quot;</span><span class="p">,</span>
            <span class="ne">UserWarning</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">nrows</span> <span class="o">&lt;=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Model training requires at least 3 observations, but the dataframe passed &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;to training has </span><span class="si">{</span><span class="n">nrows</span><span class="si">}</span><span class="s2"> rows after removing NAs.&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Sometimes this can be caused by unnecessary columns in your training data &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;which contain NAs. Make sure to remove unnecessary columns from data before &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;passing it to the function.&quot;</span><span class="p">)</span>

    <span class="c1"># an internal function for fitting model</span>
    <span class="c1"># this is wrapped into a function since we can do evaluations</span>
    <span class="n">trained_model</span> <span class="o">=</span> <span class="n">fit_ml_model</span><span class="p">(</span>
        <span class="n">df</span><span class="o">=</span><span class="n">df</span><span class="p">,</span>
        <span class="n">model_formula_str</span><span class="o">=</span><span class="n">model_formula_str</span><span class="p">,</span>
        <span class="n">fit_algorithm</span><span class="o">=</span><span class="n">fit_algorithm</span><span class="p">,</span>
        <span class="n">fit_algorithm_params</span><span class="o">=</span><span class="n">fit_algorithm_params</span><span class="p">,</span>
        <span class="n">y_col</span><span class="o">=</span><span class="n">y_col</span><span class="p">,</span>
        <span class="n">pred_cols</span><span class="o">=</span><span class="n">pred_cols</span><span class="p">,</span>
        <span class="n">min_admissible_value</span><span class="o">=</span><span class="n">min_admissible_value</span><span class="p">,</span>
        <span class="n">max_admissible_value</span><span class="o">=</span><span class="n">max_admissible_value</span><span class="p">,</span>
        <span class="n">uncertainty_dict</span><span class="o">=</span><span class="n">uncertainty_dict</span><span class="p">,</span>
        <span class="n">normalize_method</span><span class="o">=</span><span class="n">normalize_method</span><span class="p">,</span>
        <span class="n">regression_weight_col</span><span class="o">=</span><span class="n">regression_weight_col</span><span class="p">,</span>
        <span class="n">remove_intercept</span><span class="o">=</span><span class="n">remove_intercept</span><span class="p">)</span>

    <span class="c1"># we store the obtained ``y_col`` from the function in a new variable (``y_col_final``)</span>
    <span class="c1"># this is done since the input y_col could be None</span>
    <span class="c1"># in which case we extract ``y_col`` from the formula (``model_formula_str``)</span>
    <span class="n">y_col_final</span> <span class="o">=</span> <span class="n">trained_model</span><span class="p">[</span><span class="s2">&quot;y_col&quot;</span><span class="p">]</span>

    <span class="c1"># determining what should be the training and test sets</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
    <span class="n">skip_test</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">ind_train</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">ind_test</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">max</span><span class="p">(</span><span class="n">ind_train</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="nb">min</span><span class="p">(</span><span class="n">ind_test</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Test set indices should start after training set indices.&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">max</span><span class="p">(</span><span class="n">ind_test</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">n</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Testing set indices exceed the size of the dataset.&quot;</span>
                <span class="s2">&quot;Setting max index of the Test set &quot;</span>
                <span class="s2">&quot;equal to the max index of the dataset.&quot;</span><span class="p">)</span>
            <span class="n">ind_test</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">ind_test</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">]</span>
    <span class="k">elif</span> <span class="n">ind_train</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">training_fraction</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">training_fraction</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
        <span class="n">k</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="n">training_fraction</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
        <span class="n">ind_train</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">randomize_training</span><span class="p">:</span>
            <span class="n">ind_train</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">k</span><span class="p">)</span>
            <span class="n">ind_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">ind_train</span><span class="p">)</span>
        <span class="n">ind_test</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">))</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="n">ind_train</span><span class="p">))</span>
        <span class="n">ind_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">ind_test</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ind_train</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
        <span class="n">ind_test</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">skip_test</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="n">df_train</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">ind_train</span><span class="p">]</span>
    <span class="n">df_test</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">ind_test</span><span class="p">]</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">df_train</span><span class="p">[</span><span class="n">y_col_final</span><span class="p">]</span>
    <span class="n">y_test</span> <span class="o">=</span> <span class="n">df_test</span><span class="p">[</span><span class="n">y_col_final</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">skip_test</span><span class="p">:</span>
        <span class="n">y_test_pred</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">test_evaluation</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">plt_compare_test</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">y_train_pred</span> <span class="o">=</span> <span class="n">predict_ml</span><span class="p">(</span>
            <span class="n">fut_df</span><span class="o">=</span><span class="n">df</span><span class="p">,</span>
            <span class="n">trained_model</span><span class="o">=</span><span class="n">trained_model</span><span class="p">,</span>
            <span class="p">)[</span><span class="s2">&quot;fut_df&quot;</span><span class="p">][</span><span class="n">y_col_final</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

        <span class="k">def</span> <span class="nf">plt_pred</span><span class="p">():</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ind_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;full data&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ind_train</span><span class="p">,</span> <span class="n">y_train_pred</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;fit&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;index&quot;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;value&quot;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;fit on the whole dataset&quot;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># validation: fit with df_train only and predict with df_test</span>
        <span class="c1"># first remove responses from df_test</span>
        <span class="n">df_test</span><span class="p">[</span><span class="n">y_col_final</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">trained_model_tr</span> <span class="o">=</span> <span class="n">fit_ml_model</span><span class="p">(</span>
            <span class="n">df</span><span class="o">=</span><span class="n">df_train</span><span class="p">,</span>
            <span class="n">model_formula_str</span><span class="o">=</span><span class="n">model_formula_str</span><span class="p">,</span>
            <span class="n">fit_algorithm</span><span class="o">=</span><span class="n">fit_algorithm</span><span class="p">,</span>
            <span class="n">y_col</span><span class="o">=</span><span class="n">y_col</span><span class="p">,</span>
            <span class="n">pred_cols</span><span class="o">=</span><span class="n">pred_cols</span><span class="p">,</span>
            <span class="n">min_admissible_value</span><span class="o">=</span><span class="n">min_admissible_value</span><span class="p">,</span>
            <span class="n">max_admissible_value</span><span class="o">=</span><span class="n">max_admissible_value</span><span class="p">,</span>
            <span class="n">uncertainty_dict</span><span class="o">=</span><span class="n">uncertainty_dict</span><span class="p">,</span>
            <span class="n">normalize_method</span><span class="o">=</span><span class="n">normalize_method</span><span class="p">,</span>
            <span class="n">regression_weight_col</span><span class="o">=</span><span class="n">regression_weight_col</span><span class="p">,</span>
            <span class="n">remove_intercept</span><span class="o">=</span><span class="n">remove_intercept</span><span class="p">)</span>

        <span class="n">y_train_pred</span> <span class="o">=</span> <span class="n">predict_ml</span><span class="p">(</span>
            <span class="n">fut_df</span><span class="o">=</span><span class="n">df_train</span><span class="p">,</span>
            <span class="n">trained_model</span><span class="o">=</span><span class="n">trained_model_tr</span><span class="p">)[</span><span class="s2">&quot;fut_df&quot;</span><span class="p">][</span><span class="n">y_col_final</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

        <span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">predict_ml</span><span class="p">(</span>
            <span class="n">fut_df</span><span class="o">=</span><span class="n">df_test</span><span class="p">,</span>
            <span class="n">trained_model</span><span class="o">=</span><span class="n">trained_model_tr</span><span class="p">)[</span><span class="s2">&quot;fut_df&quot;</span><span class="p">][</span><span class="n">y_col_final</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

        <span class="n">test_evaluation</span> <span class="o">=</span> <span class="n">calc_pred_err</span><span class="p">(</span>
            <span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span>
            <span class="n">y_pred</span><span class="o">=</span><span class="n">y_test_pred</span><span class="p">)</span>

        <span class="n">test_evaluation</span><span class="p">[</span><span class="n">R2_null_model_score</span><span class="p">]</span> <span class="o">=</span> <span class="n">r2_null_model_score</span><span class="p">(</span>
            <span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span>
            <span class="n">y_pred</span><span class="o">=</span><span class="n">y_test_pred</span><span class="p">,</span>
            <span class="n">y_pred_null</span><span class="o">=</span><span class="n">y_train</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span>
            <span class="n">y_train</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">plt_compare_test</span><span class="p">():</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;observed&quot;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;predicted&quot;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;test set&quot;</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">plt_pred</span><span class="p">():</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ind_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ind_train</span><span class="p">,</span> <span class="n">y_train_pred</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;fit&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ind_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;observed test set&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ind_test</span><span class="p">,</span> <span class="n">y_test_pred</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;predicted test set&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;index&quot;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;value&quot;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;training and test fits&quot;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="n">training_evaluation</span> <span class="o">=</span> <span class="n">calc_pred_err</span><span class="p">(</span>
        <span class="n">y_true</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span>
        <span class="n">y_pred</span><span class="o">=</span><span class="n">y_train_pred</span><span class="p">)</span>
    <span class="n">training_evaluation</span><span class="p">[</span><span class="n">R2_null_model_score</span><span class="p">]</span> <span class="o">=</span> <span class="n">r2_null_model_score</span><span class="p">(</span>
        <span class="n">y_true</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span>
        <span class="n">y_pred</span><span class="o">=</span><span class="n">y_train_pred</span><span class="p">,</span>
        <span class="n">y_pred_null</span><span class="o">=</span><span class="n">y_train</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span>
        <span class="n">y_train</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

    <span class="n">trained_model</span><span class="p">[</span><span class="s2">&quot;summary&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">trained_model</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">y_col_final</span><span class="p">]</span>
    <span class="n">trained_model</span><span class="p">[</span><span class="s2">&quot;y_train&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_train</span>
    <span class="n">trained_model</span><span class="p">[</span><span class="s2">&quot;y_train_pred&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_train_pred</span>
    <span class="n">trained_model</span><span class="p">[</span><span class="s2">&quot;training_evaluation&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">training_evaluation</span>
    <span class="n">trained_model</span><span class="p">[</span><span class="s2">&quot;y_test&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_test</span>
    <span class="n">trained_model</span><span class="p">[</span><span class="s2">&quot;y_test_pred&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_test_pred</span>
    <span class="n">trained_model</span><span class="p">[</span><span class="s2">&quot;test_evaluation&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">test_evaluation</span>
    <span class="n">trained_model</span><span class="p">[</span><span class="s2">&quot;plt_compare_test&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">plt_compare_test</span>
    <span class="n">trained_model</span><span class="p">[</span><span class="s2">&quot;plt_pred&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">plt_pred</span>

    <span class="k">return</span> <span class="n">trained_model</span></div>


<span class="k">def</span> <span class="nf">breakdown_regression_based_prediction</span><span class="p">(</span>
        <span class="n">trained_model</span><span class="p">,</span>
        <span class="n">x_mat</span><span class="p">,</span>
        <span class="n">grouping_regex_patterns_dict</span><span class="p">,</span>
        <span class="n">remainder_group_name</span><span class="o">=</span><span class="s2">&quot;OTHER&quot;</span><span class="p">,</span>
        <span class="n">center_components</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">denominator</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">index_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">index_col</span><span class="o">=</span><span class="s2">&quot;index_col&quot;</span><span class="p">,</span>
        <span class="n">plt_title</span><span class="o">=</span><span class="s2">&quot;prediction breakdown&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Given a regression based ML model (``ml_model``) and a design matrix</span>
<span class="sd">    (``x_mat``), and a string based grouping rule (``grouping_regex_patterns_dict``)</span>
<span class="sd">    for the design matrix columnns, constructs a dataframe with columns corresponding</span>
<span class="sd">    to the weighted (according to ML model regression coefficient) sum of the columns in each group.</span>
<span class="sd">    Note that if a variable/column is already picked in a step, it will be taken</span>
<span class="sd">    out from the columns list and will not appear in next groups.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    trained_model : `dict`</span>
<span class="sd">        A trained machine-learning model which includes items:</span>

<span class="sd">        - ml_model : `sklearn.base.BaseEstimator`</span>
<span class="sd">            sklearn ML estimator/model of various form.</span>
<span class="sd">            We require this object to have ``.coef_`` and ``.intercept`` attributes.</span>
<span class="sd">        - y_mean : `float`</span>
<span class="sd">            Observed mean of the response</span>
<span class="sd">        - y_std : `float`</span>
<span class="sd">            Observed standard deviation of the response</span>

<span class="sd">    x_mat :`pandas.DataFrame`</span>
<span class="sd">        Design matrix of the regression model</span>
<span class="sd">    grouping_regex_patterns_dict : `dict` {`str`: `str`}</span>
<span class="sd">        A dictionary with group names as keys and regexes as values.</span>
<span class="sd">        This dictinary is used to partition the columns into various groups</span>
<span class="sd">    remainder_group_name : `str`, default &quot;OTHER&quot;</span>
<span class="sd">        In case some columns are left and not assigned to any groups, a group</span>
<span class="sd">        with this name will be added to breakdown dataframe and includes the</span>
<span class="sd">        weighted some of the remaining columns.</span>
<span class="sd">    center_components : `bool`, default False</span>
<span class="sd">        It determines if components should be centered at their mean and the mean</span>
<span class="sd">        be added to the intercept. More concretely, if a componet is &quot;x&quot; then it will</span>
<span class="sd">        be mapped to &quot;x - mean(x)&quot;; and &quot;mean(x)&quot; will be added to the intercept so</span>
<span class="sd">        that the sum of the components remains the same.</span>
<span class="sd">    denominator : `str`, default None</span>
<span class="sd">        If not None, it will specify a way to divide the components. There are</span>
<span class="sd">        two options implemented:</span>

<span class="sd">        - &quot;abs_y_mean&quot; : `float`</span>
<span class="sd">            The absolute value of the observed mean of the response</span>
<span class="sd">        - &quot;y_std&quot; : `float`</span>
<span class="sd">            The standard deviation of the observed response</span>

<span class="sd">        This will be useful if we want to make the components scale free.</span>
<span class="sd">        Dividing by the absolute mean value of the response, is particularly</span>
<span class="sd">        useful to understand how much impact each component has for an average</span>
<span class="sd">        response.</span>

<span class="sd">    index_values : `list`, default None</span>
<span class="sd">        The values added as index which can of any types that can be used for</span>
<span class="sd">        plotting the x axis in plotly eg `int` or `datetime`.</span>
<span class="sd">        This is useful for plotting or if later this data to be joined</span>
<span class="sd">        with other data. For example in forecasting context timestamps can be added.</span>
<span class="sd">    index_col : `str`, default &quot;index_col&quot;</span>
<span class="sd">        The name of the added column to breakdown data to keep track of index</span>
<span class="sd">    plt_title : `str`, default &quot;prediction breakdown&quot;</span>
<span class="sd">        The title of generated plot</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    result : `dict`</span>
<span class="sd">        A dictionary with the following keys.</span>

<span class="sd">        - &quot;breakdown_df&quot; : `pandas.DataFrame`</span>
<span class="sd">            A dataframe which includes the sums for each group / component</span>
<span class="sd">        - &quot;breakdown_df_with_index_col&quot; : `pandas.DataFrame`</span>
<span class="sd">            Same as ``breakdown_df`` with an added column to keep track of index</span>
<span class="sd">        - &quot;breakdown_fig&quot; : `plotly.graph_objs._figure.Figure`</span>
<span class="sd">            plotly plot overlaying various components</span>
<span class="sd">        - &quot;column_grouping_result&quot; : `dict`</span>
<span class="sd">            A dictionary which includes information for the generated groups.</span>
<span class="sd">            See `~greykite.common.python_utils.group_strs_with_regex_patterns`</span>
<span class="sd">            for more details.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">index_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">index_values</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_mat</span><span class="p">),</span> <span class="s2">&quot;the number of indices must match the size of data&quot;</span>

    <span class="n">ml_model</span> <span class="o">=</span> <span class="n">trained_model</span><span class="p">[</span><span class="s2">&quot;ml_model&quot;</span><span class="p">]</span>
    <span class="c1"># The dataframe which includes the group sums</span>
    <span class="n">breakdown_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">ml_model_coef</span> <span class="o">=</span> <span class="n">ml_model</span><span class="o">.</span><span class="n">coef_</span>
    <span class="n">intercept</span> <span class="o">=</span> <span class="n">ml_model</span><span class="o">.</span><span class="n">intercept_</span>
    <span class="n">x_mat_weighted</span> <span class="o">=</span> <span class="n">x_mat</span> <span class="o">*</span> <span class="n">ml_model_coef</span>
    <span class="n">data_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_mat</span><span class="p">)</span>
    <span class="n">cols</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">x_mat</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

    <span class="n">breakdown_df</span><span class="p">[</span><span class="s2">&quot;Intercept&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">intercept</span><span class="p">,</span> <span class="n">data_len</span><span class="p">)</span>
    <span class="k">if</span> <span class="s2">&quot;Intercept&quot;</span> <span class="ow">in</span> <span class="n">cols</span><span class="p">:</span>
        <span class="n">breakdown_df</span><span class="p">[</span><span class="s2">&quot;Intercept&quot;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">x_mat_weighted</span><span class="p">[</span><span class="s2">&quot;Intercept&quot;</span><span class="p">]</span>

    <span class="c1"># If Intercept appear in the columns, we remove it</span>
    <span class="c1"># Note that this column was utilized and added to intercept</span>
    <span class="k">if</span> <span class="s2">&quot;Intercept&quot;</span> <span class="ow">in</span> <span class="n">cols</span><span class="p">:</span>
        <span class="k">del</span> <span class="n">x_mat_weighted</span><span class="p">[</span><span class="s2">&quot;Intercept&quot;</span><span class="p">]</span>
        <span class="n">cols</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s2">&quot;Intercept&quot;</span><span class="p">)</span>

    <span class="n">regex_patterns</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">grouping_regex_patterns_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
    <span class="n">group_names</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">grouping_regex_patterns_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

    <span class="n">column_grouping_result</span> <span class="o">=</span> <span class="n">group_strs_with_regex_patterns</span><span class="p">(</span>
        <span class="n">strings</span><span class="o">=</span><span class="n">cols</span><span class="p">,</span>
        <span class="n">regex_patterns</span><span class="o">=</span><span class="n">regex_patterns</span><span class="p">)</span>

    <span class="n">col_groups</span> <span class="o">=</span> <span class="n">column_grouping_result</span><span class="p">[</span><span class="s2">&quot;str_groups&quot;</span><span class="p">]</span>
    <span class="n">remainder</span> <span class="o">=</span> <span class="n">column_grouping_result</span><span class="p">[</span><span class="s2">&quot;remainder&quot;</span><span class="p">]</span>

    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">col_groups</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">grouping_regex_patterns_dict</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">group_name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">group_names</span><span class="p">):</span>
        <span class="n">group_elements</span> <span class="o">=</span> <span class="n">col_groups</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">group_elements</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">breakdown_df</span><span class="p">[</span><span class="n">group_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">x_mat_weighted</span><span class="p">[</span><span class="n">group_elements</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">breakdown_df</span><span class="p">[</span><span class="n">group_name</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">remainder</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">breakdown_df</span><span class="p">[</span><span class="n">remainder_group_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">x_mat_weighted</span><span class="p">[</span><span class="n">remainder</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">group_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">remainder_group_name</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">center_components</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">group_names</span><span class="p">:</span>
            <span class="n">col_mean</span> <span class="o">=</span> <span class="n">breakdown_df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
            <span class="n">breakdown_df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">+=</span> <span class="o">-</span><span class="n">col_mean</span>
            <span class="n">breakdown_df</span><span class="p">[</span><span class="s2">&quot;Intercept&quot;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">col_mean</span>

    <span class="k">if</span> <span class="n">denominator</span> <span class="o">==</span> <span class="s2">&quot;abs_y_mean&quot;</span><span class="p">:</span>
        <span class="n">d</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">trained_model</span><span class="p">[</span><span class="s2">&quot;y_mean&quot;</span><span class="p">])</span>
    <span class="k">elif</span> <span class="n">denominator</span> <span class="o">==</span> <span class="s2">&quot;y_std&quot;</span><span class="p">:</span>
        <span class="n">d</span> <span class="o">=</span> <span class="n">trained_model</span><span class="p">[</span><span class="s2">&quot;y_std&quot;</span><span class="p">]</span>
    <span class="k">elif</span> <span class="n">denominator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">d</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">denominator</span><span class="si">}</span><span class="s2"> is not an admissable denominator&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">group_names</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;Intercept&quot;</span><span class="p">]:</span>
        <span class="n">breakdown_df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">/=</span> <span class="n">d</span>

    <span class="k">if</span> <span class="n">index_values</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">index_values</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">breakdown_df</span><span class="p">))</span>

    <span class="n">breakdown_df_with_index_col</span> <span class="o">=</span> <span class="n">breakdown_df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">breakdown_df_with_index_col</span><span class="p">[</span><span class="n">index_col</span><span class="p">]</span> <span class="o">=</span> <span class="n">index_values</span>

    <span class="n">breakdown_fig</span> <span class="o">=</span> <span class="n">plot_multivariate</span><span class="p">(</span>
        <span class="n">df</span><span class="o">=</span><span class="n">breakdown_df_with_index_col</span><span class="p">,</span>
        <span class="n">x_col</span><span class="o">=</span><span class="n">index_col</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="n">plt_title</span><span class="p">,</span>
        <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;component&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;breakdown_df&quot;</span><span class="p">:</span> <span class="n">breakdown_df</span><span class="p">,</span>
        <span class="s2">&quot;breakdown_df_with_index_col&quot;</span><span class="p">:</span> <span class="n">breakdown_df_with_index_col</span><span class="p">,</span>
        <span class="s2">&quot;breakdown_fig&quot;</span><span class="p">:</span> <span class="n">breakdown_fig</span><span class="p">,</span>
        <span class="s2">&quot;column_grouping_result&quot;</span><span class="p">:</span> <span class="n">column_grouping_result</span>
    <span class="p">}</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, LinkedIn.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>